{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f488a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset & DataLoader helpers\n",
    "# -----------------------------\n",
    "\n",
    "class FoGWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for window-level FoG data.\n",
    "\n",
    "    Expects a DataFrame with columns:\n",
    "        - 'sequence'     : (T, 3) accel window (list/array/string)\n",
    "        - 'window_label' : 0/1\n",
    "        - 'subject'      : patient ID (not used in __getitem__, but kept in df)\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def _parse_sequence(self, seq_obj):\n",
    "        # Handles numpy array, python list, or string from CSV\n",
    "        if isinstance(seq_obj, np.ndarray):\n",
    "            arr = seq_obj.astype(np.float32)\n",
    "        elif isinstance(seq_obj, list):\n",
    "            arr = np.asarray(seq_obj, dtype=np.float32)\n",
    "        else:\n",
    "            # assume string like \"[[...], [...], ...]\"\n",
    "            arr = np.array(ast.literal_eval(seq_obj), dtype=np.float32)\n",
    "\n",
    "        # Ensure shape (T, 3)\n",
    "        if arr.ndim == 1:\n",
    "            arr = arr.reshape(-1, 1)\n",
    "        return arr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq = self._parse_sequence(row['sequence'])  # (T, 3)\n",
    "        label = float(row['window_label'])\n",
    "\n",
    "        x = torch.from_numpy(seq)               # (T, 3)\n",
    "        y = torch.tensor(label, dtype=torch.float32)  # scalar 0/1\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def make_dataloader(df: pd.DataFrame,\n",
    "                    batch_size: int = 64,\n",
    "                    shuffle: bool = True,\n",
    "                    num_workers: int = 0) -> DataLoader:\n",
    "    dataset = FoGWindowDataset(df)\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        num_workers=num_workers)\n",
    "    return loader\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Patient-independent folds\n",
    "# -----------------------------\n",
    "\n",
    "def create_group_kfold_splits(df: pd.DataFrame,\n",
    "                              n_splits: int = 5,\n",
    "                              random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Patient-independent K-fold splits using GroupKFold on 'subject'.\n",
    "\n",
    "    Returns a list of (train_idx, val_idx) index arrays.\n",
    "    \"\"\"\n",
    "    groups = df['subject'].values\n",
    "    y = df['window_label'].values\n",
    "    X = np.arange(len(df))\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    # GroupKFold is deterministic; we can shuffle subjects before if needed:\n",
    "    # but simplest is to just use GroupKFold directly.\n",
    "    splits = list(gkf.split(X, y, groups))\n",
    "    return splits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics helper\n",
    "# -----------------------------\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray,\n",
    "                    y_pred_probs: np.ndarray,\n",
    "                    threshold: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, recall, F1 for binary classification.\n",
    "    \"\"\"\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='binary', zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'accuracy': float(acc),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1': float(f1),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# One epoch train / eval\n",
    "# -----------------------------\n",
    "\n",
    "def run_epoch(model: nn.Module,\n",
    "              loader: DataLoader,\n",
    "              criterion,\n",
    "              device: torch.device,\n",
    "              optimizer=None) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    If optimizer is provided: training mode.\n",
    "    Otherwise: evaluation mode.\n",
    "\n",
    "    Returns dict: loss, accuracy, precision, recall, f1\n",
    "    \"\"\"\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "    else:\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "    all_losses = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)              # (batch, T, 3)\n",
    "        y = y.to(device)              # (batch,)\n",
    "\n",
    "        logits = model(x)             # (batch,)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        all_probs.append(probs.detach().cpu().numpy())\n",
    "        all_labels.append(y.detach().cpu().numpy())\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    metrics = compute_metrics(all_labels, all_probs)\n",
    "    metrics['loss'] = float(np.mean(all_losses))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training one fold (with early stopping & scheduler)\n",
    "# -----------------------------\n",
    "\n",
    "def train_one_fold(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    input_dim: int = 3,\n",
    "    hidden_dim: int = 128,\n",
    "    num_layers: int = 2,\n",
    "    bidirectional: bool = False,\n",
    "    dropout: float = 0.0,\n",
    "    batch_size: int = 16,\n",
    "    num_epochs: int = 50,\n",
    "    lr: float = 1e-4,\n",
    "    weight_decay: float = 1e-4,\n",
    "    early_stopping_patience: int = 7,\n",
    "    device: str = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train LSTM on one fold with early stopping on val F1 and\n",
    "    ReduceLROnPlateau scheduler on val loss.\n",
    "\n",
    "    Returns:\n",
    "        dict with:\n",
    "            - 'best_state_dict'\n",
    "            - 'history' : list of per-epoch metrics\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = make_dataloader(train_df, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = make_dataloader(val_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model, loss, optimizer, scheduler\n",
    "    #model =  ParallelCNNLSTMTransformer().to(device)\n",
    "    # ---------------------------\n",
    "    # Quick test\n",
    "    # ---------------------------\n",
    "\n",
    "    B = 4\n",
    "    T = 256\n",
    "    C = 3\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = LSTMTransformer().to(device)\n",
    "\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    #criterion = WeightedBCEWithLogitsLoss()\n",
    "    #criterion = FocalTverskyLoss(gamma=0.75, alpha=0.7)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    best_val_f1 = -np.inf\n",
    "    best_state_dict = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    history = []  # list of dicts per epoch\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # --- Train ---\n",
    "        train_metrics = run_epoch(model, train_loader, criterion, device, optimizer)\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_metrics = run_epoch(model, val_loader, criterion, device, optimizer=None)\n",
    "\n",
    "        # Step scheduler on validation loss\n",
    "        scheduler.step(val_metrics['loss'])\n",
    "\n",
    "        # Early stopping on val F1\n",
    "        val_f1 = val_metrics['f1']\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Record metrics\n",
    "        epoch_record = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_metrics['loss'],\n",
    "            'train_accuracy': train_metrics['accuracy'],\n",
    "            'train_precision': train_metrics['precision'],\n",
    "            'train_recall': train_metrics['recall'],\n",
    "            'train_f1': train_metrics['f1'],\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_accuracy': val_metrics['accuracy'],\n",
    "            'val_precision': val_metrics['precision'],\n",
    "            'val_recall': val_metrics['recall'],\n",
    "            'val_f1': val_metrics['f1'],\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "        history.append(epoch_record)\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch:02d}] \"\n",
    "            f\"Train Loss={train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, F1={train_metrics['f1']:.4f} | \"\n",
    "\n",
    "            f\"Val Loss={val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f} F1={val_metrics['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no val F1 improvement for \"\n",
    "                  f\"{early_stopping_patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'best_state_dict': best_state_dict,\n",
    "        'history': history,\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize_and_save_cv_results(\n",
    "        results: Dict[str, Any],\n",
    "        output_prefix: str = \"cv_results\",\n",
    "        save_best_model_path: str = \"best_model_overall.pt\",\n",
    "        save_all_folds_dir: str = \"all_best_models\"):\n",
    "    \"\"\"\n",
    "    - Extract best metrics from each fold\n",
    "    - Compute mean and std\n",
    "    - Save fold-wise results and summary to Excel\n",
    "    - Save ALL best model weights for all folds\n",
    "    - Save the global best model across folds\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    os.makedirs(save_all_folds_dir, exist_ok=True)\n",
    "\n",
    "    fold_best_rows = []\n",
    "    global_best_f1 = -np.inf\n",
    "    global_best_state_dict = None\n",
    "    global_best_fold = None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract best epoch per fold\n",
    "    # -----------------------------\n",
    "    for fold_data in results['folds']:\n",
    "        fold_idx = fold_data['fold_idx']\n",
    "        history = fold_data['history']\n",
    "\n",
    "        # Best epoch based on val_f1\n",
    "        best_epoch_record = max(history, key=lambda x: x['val_f1'])\n",
    "\n",
    "        row = {\n",
    "            'fold': fold_idx,\n",
    "            'val_accuracy': best_epoch_record['val_accuracy'],\n",
    "            'val_precision': best_epoch_record['val_precision'],\n",
    "            'val_recall': best_epoch_record['val_recall'],\n",
    "            'val_f1': best_epoch_record['val_f1'],\n",
    "            'val_loss': best_epoch_record['val_loss'],\n",
    "        }\n",
    "\n",
    "        fold_best_rows.append(row)\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # Save THIS FOLD'S best model separately\n",
    "        # ---------------------------------------\n",
    "        fold_model_path = os.path.join(\n",
    "            save_all_folds_dir, f\"best_model_fold_{fold_idx}.pt\"\n",
    "        )\n",
    "        torch.save(fold_data['best_state_dict'], fold_model_path)\n",
    "        print(f\" Saved best model for Fold {fold_idx} â†’ {fold_model_path}\")\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # Track the GLOBAL best model\n",
    "        # ---------------------------------------\n",
    "        if best_epoch_record['val_f1'] > global_best_f1:\n",
    "            global_best_f1 = best_epoch_record['val_f1']\n",
    "            global_best_state_dict = fold_data['best_state_dict']\n",
    "            global_best_fold = fold_idx\n",
    "\n",
    "    df_folds = pd.DataFrame(fold_best_rows)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Mean & Std\n",
    "    # -----------------------------\n",
    "    summary = {}\n",
    "    for col in ['val_accuracy', 'val_precision', 'val_recall', 'val_f1', 'val_loss']:\n",
    "        summary[f'{col}_mean'] = df_folds[col].mean()\n",
    "        summary[f'{col}_std'] = df_folds[col].std()\n",
    "\n",
    "    df_summary = pd.DataFrame([summary])\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save Excel files\n",
    "    # -----------------------------\n",
    "    folds_excel = f\"{output_prefix}_folds.xlsx\"\n",
    "    summary_excel = f\"{output_prefix}_summary.xlsx\"\n",
    "\n",
    "    df_folds.to_excel(folds_excel, index=False)\n",
    "    df_summary.to_excel(summary_excel, index=False)\n",
    "\n",
    "    print(f\" Fold metrics saved to Excel: {folds_excel}\")\n",
    "    print(f\" Summary metrics saved to Excel: {summary_excel}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save global best model\n",
    "    # -----------------------------\n",
    "    if global_best_state_dict is not None:\n",
    "        torch.save(global_best_state_dict, save_best_model_path)\n",
    "        print(f\"ðŸ† Global best model (Fold {global_best_fold}) saved â†’ {save_best_model_path}\")\n",
    "\n",
    "    return {\n",
    "        'fold_metrics_df': df_folds,\n",
    "        'summary_df': df_summary,\n",
    "        'best_fold': global_best_fold,\n",
    "        'best_f1': global_best_f1\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Full K-fold cross-validation\n",
    "# -----------------------------\n",
    "\n",
    "def cross_validate_patient_independent(\n",
    "    df: pd.DataFrame,\n",
    "    n_splits: int = 5,\n",
    "    random_state: int = 42,\n",
    "    **train_kwargs\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run patient-independent K-fold cross-validation.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          'folds': [\n",
    "            {\n",
    "              'fold_idx': 0,\n",
    "              'train_subjects': [...],\n",
    "              'val_subjects': [...],\n",
    "              'history': [...],           # list of per-epoch dicts\n",
    "              'best_state_dict': {...},\n",
    "            },\n",
    "            ...\n",
    "          ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    splits = create_group_kfold_splits(df, n_splits=n_splits, random_state=random_state)\n",
    "\n",
    "    results = {'folds': []}\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(splits):\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_subjects = sorted(train_df['subject'].unique())\n",
    "        val_subjects = sorted(val_df['subject'].unique())\n",
    "\n",
    "        print(f\"Train subjects (n={len(train_subjects)}): {train_subjects}\")\n",
    "        print(f\"Val subjects   (n={len(val_subjects)}): {val_subjects}\")\n",
    "        print(f\"Train windows: {len(train_df)}, Val windows: {len(val_df)}\")\n",
    "\n",
    "        fold_result = train_one_fold(\n",
    "            train_df=train_df,\n",
    "            val_df=val_df,\n",
    "            **train_kwargs\n",
    "        )\n",
    "\n",
    "        results['folds'].append({\n",
    "            'fold_idx': fold_idx,\n",
    "            'train_subjects': train_subjects,\n",
    "            'val_subjects': val_subjects,\n",
    "            'history': fold_result['history'],\n",
    "            'best_state_dict': fold_result['best_state_dict'],\n",
    "        })\n",
    "    #save the results:\n",
    "    summary = summarize_and_save_cv_results(\n",
    "        results,\n",
    "        output_prefix=\"fog_cv\",\n",
    "        save_best_model_path=\"best_fog_model.pt\"\n",
    "    )\n",
    "\n",
    "    results['summary'] = summary\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ecf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "# --- 4. AttentionLayer (No Change) ---\n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Attention mechanism for global context pooling.\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, hidden_dim)\n",
    "        scores = torch.relu(self.attention(x)) \n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1) \n",
    "        context = torch.sum(x * weights.unsqueeze(-1), dim=1) \n",
    "        return context\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# LSTM branch (stacked LSTM)\n",
    "# ---------------------------\n",
    "class LSTMBranch(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_size=128, num_layers=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=in_channels, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:  (B, T, C)\n",
    "        out, _ = self.lstm(x)  # (B, T, hidden)\n",
    "        return out  # (B, T, hidden)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Shared Transformer (fusion) with optional causal mask\n",
    "# ---------------------------\n",
    "class SharedTransformerFusion(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, ff_mult=2, dropout=0.1, use_causal_mask=False):\n",
    "        \"\"\"\n",
    "        embed_dim: embedding dimension per token (here token dim is 2E because we concat projections)\n",
    "        use_causal_mask: if True, transformer attention is masked so each time step attends only to the past (and itself)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_causal_mask = use_causal_mask\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * ff_mult),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim * ff_mult, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "\n",
    "    def _causal_mask(self, T, device):\n",
    "        # returns additive mask of shape (T, T) with -inf in upper triangle (j > i)\n",
    "        mask = torch.triu(torch.ones(T, T, device=device) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, embed_dim)\n",
    "        B, T, D = x.shape\n",
    "        attn_mask = None\n",
    "        if self.use_causal_mask:\n",
    "            attn_mask = self._causal_mask(T, x.device)  # shape (T, T) acceptable by MultiheadAttention\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
    "        x = self.ln1(x + attn_out)\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.ln2(x + ff_out)\n",
    "        return x  # (B, T, embed_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 seq_len=256,\n",
    "                 in_channels=3,\n",
    "                 tcn_channels=[32, 64, 128, 128],\n",
    "                 tcn_kernel=3,\n",
    "                 lstm_hidden=96,\n",
    "                 lstm_layers=2,\n",
    "                 proj_embed=64,          # E\n",
    "                 transformer_heads=4,\n",
    "                 transformer_ffmult=2,\n",
    "                 transformer_dropout=0.2,\n",
    "                 use_causal_transformer=True):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.lstm = LSTMBranch(in_channels=in_channels, hidden_size=lstm_hidden,\n",
    "                               num_layers=lstm_layers)\n",
    "\n",
    "\n",
    "        self.proj_lstm = nn.Linear(lstm_hidden, proj_embed)\n",
    "\n",
    "        # === Shared Transformer: receives 2E features ===\n",
    "        embed_dim = 1 * proj_embed\n",
    "        self.shared_transformer = SharedTransformerFusion(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=transformer_heads,\n",
    "            ff_mult=transformer_ffmult,\n",
    "            dropout=transformer_dropout,\n",
    "            use_causal_mask=use_causal_transformer\n",
    "        )\n",
    "\n",
    "        # ===  Attention pooling instead of GlobalAveragePool ===\n",
    "        self.attention = AttentionLayer(hidden_dim=embed_dim)\n",
    "\n",
    "        # === Final classifier ===\n",
    "        self.pool_dropout = nn.Dropout(0.2)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "\n",
    "\n",
    "        l = self.lstm(x)           # (B, T, lstm_hidden)\n",
    "\n",
    "\n",
    "        p_l = self.proj_lstm(l)\n",
    "\n",
    "\n",
    "\n",
    "        # Transformer encoding\n",
    "        z = self.shared_transformer(p_l)    # (B, T, 2E)\n",
    "\n",
    "        # ===  Attention pooling (replace mean pooling) ===\n",
    "        context = self.attention(z)           # (B, 2E)\n",
    "        context = self.pool_dropout(context)\n",
    "\n",
    "        out = self.mlp(context).squeeze(-1)   # (B,)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c179b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Student\\\\Desktop\\\\Abouhashem\\\\DeepLearningProject\\\\'\n",
    "train_df = pd.read_pickle(path+ \"FoG_windows_train.pkl\")\n",
    "test_df  = pd.read_pickle(path+\"FoG_windows_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad74cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. AttentionLayer (No Change) ---\n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Attention mechanism for global context pooling.\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, hidden_dim)\n",
    "        scores = torch.relu(self.attention(x)) \n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1) \n",
    "        context = torch.sum(x * weights.unsqueeze(-1), dim=1) \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17cc619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Fold 1/5\n",
      "Train subjects (n=32): ['07285e', '194d1d', '220a17', '231c3b', '24a59d', '251738', '2a39f8', '2c98f7', '31d269', '364459', '3b2403', '48fd62', '4b39ac', '4ca9b3', '4f13b4', '516a67', '54ee6e', '66341b', '7688c1', '79011a', '7eb666', '7fcee9', '8db7dd', '93f49f', 'a03db7', 'bc3908', 'c85fdf', 'c8e721', 'd8836b', 'd9312a', 'e8919c', 'f2c8aa']\n",
      "Val subjects   (n=2): ['2d57c2', '87174c']\n",
      "Train windows: 15896, Val windows: 3984\n",
      "[Epoch 01] Train Loss=0.5777, Acc: 0.7262, F1=0.0299 | Val Loss=1.2926, Acc: 0.1057 F1=0.0034\n",
      "[Epoch 02] Train Loss=0.5314, Acc: 0.7286, F1=0.2340 | Val Loss=1.3195, Acc: 0.1059 F1=0.0034\n",
      "[Epoch 03] Train Loss=0.5121, Acc: 0.7280, F1=0.2709 | Val Loss=1.0666, Acc: 0.2718 F1=0.3432\n",
      "[Epoch 04] Train Loss=0.4721, Acc: 0.7474, F1=0.4155 | Val Loss=0.9728, Acc: 0.3697 F1=0.4627\n",
      "[Epoch 05] Train Loss=0.4449, Acc: 0.7766, F1=0.4826 | Val Loss=0.7892, Acc: 0.5811 F1=0.7016\n",
      "[Epoch 06] Train Loss=0.4241, Acc: 0.7960, F1=0.5767 | Val Loss=0.6715, Acc: 0.6815 F1=0.7892\n",
      "[Epoch 07] Train Loss=0.4030, Acc: 0.8120, F1=0.6284 | Val Loss=0.7175, Acc: 0.6514 F1=0.7624\n",
      "[Epoch 08] Train Loss=0.3944, Acc: 0.8188, F1=0.6439 | Val Loss=0.6106, Acc: 0.7349 F1=0.8297\n",
      "[Epoch 09] Train Loss=0.3826, Acc: 0.8269, F1=0.6646 | Val Loss=0.7429, Acc: 0.6483 F1=0.7604\n",
      "[Epoch 10] Train Loss=0.3738, Acc: 0.8300, F1=0.6707 | Val Loss=0.4950, Acc: 0.7912 F1=0.8708\n",
      "[Epoch 11] Train Loss=0.3638, Acc: 0.8376, F1=0.6880 | Val Loss=0.5999, Acc: 0.7221 F1=0.8187\n",
      "[Epoch 12] Train Loss=0.3527, Acc: 0.8456, F1=0.7015 | Val Loss=0.5056, Acc: 0.7894 F1=0.8694\n",
      "[Epoch 13] Train Loss=0.3460, Acc: 0.8488, F1=0.7097 | Val Loss=0.5044, Acc: 0.7962 F1=0.8732\n",
      "[Epoch 14] Train Loss=0.3397, Acc: 0.8506, F1=0.7128 | Val Loss=0.5292, Acc: 0.7658 F1=0.8512\n",
      "[Epoch 15] Train Loss=0.3293, Acc: 0.8552, F1=0.7202 | Val Loss=0.4466, Acc: 0.8100 F1=0.8832\n",
      "[Epoch 16] Train Loss=0.3205, Acc: 0.8595, F1=0.7280 | Val Loss=0.8020, Acc: 0.6348 F1=0.7458\n",
      "[Epoch 17] Train Loss=0.3057, Acc: 0.8666, F1=0.7422 | Val Loss=0.3553, Acc: 0.8514 F1=0.9108\n",
      "[Epoch 18] Train Loss=0.2986, Acc: 0.8731, F1=0.7543 | Val Loss=0.3955, Acc: 0.8286 F1=0.8954\n",
      "[Epoch 19] Train Loss=0.2888, Acc: 0.8763, F1=0.7597 | Val Loss=0.3618, Acc: 0.8562 F1=0.9138\n",
      "[Epoch 20] Train Loss=0.2863, Acc: 0.8764, F1=0.7624 | Val Loss=0.3966, Acc: 0.8288 F1=0.8957\n",
      "[Epoch 21] Train Loss=0.2818, Acc: 0.8799, F1=0.7684 | Val Loss=0.4922, Acc: 0.7784 F1=0.8603\n",
      "[Epoch 22] Train Loss=0.2781, Acc: 0.8810, F1=0.7704 | Val Loss=0.5637, Acc: 0.7477 F1=0.8377\n",
      "[Epoch 23] Train Loss=0.2741, Acc: 0.8826, F1=0.7740 | Val Loss=0.3996, Acc: 0.8341 F1=0.8992\n",
      "[Epoch 24] Train Loss=0.2651, Acc: 0.8871, F1=0.7826 | Val Loss=0.3541, Acc: 0.8562 F1=0.9140\n",
      "[Epoch 25] Train Loss=0.2632, Acc: 0.8893, F1=0.7876 | Val Loss=0.3764, Acc: 0.8406 F1=0.9035\n",
      "[Epoch 26] Train Loss=0.2611, Acc: 0.8902, F1=0.7903 | Val Loss=0.4195, Acc: 0.8178 F1=0.8879\n",
      "[Epoch 27] Train Loss=0.2592, Acc: 0.8912, F1=0.7917 | Val Loss=0.4445, Acc: 0.8082 F1=0.8813\n",
      "[Epoch 28] Train Loss=0.2565, Acc: 0.8931, F1=0.7954 | Val Loss=0.5096, Acc: 0.7761 F1=0.8582\n",
      "[Epoch 29] Train Loss=0.2556, Acc: 0.8924, F1=0.7943 | Val Loss=0.4224, Acc: 0.8085 F1=0.8817\n",
      "[Epoch 30] Train Loss=0.2550, Acc: 0.8924, F1=0.7944 | Val Loss=0.4181, Acc: 0.8183 F1=0.8884\n",
      "[Epoch 31] Train Loss=0.2496, Acc: 0.8931, F1=0.7966 | Val Loss=0.4074, Acc: 0.8243 F1=0.8923\n",
      "Early stopping at epoch 31 (no val F1 improvement for 7 epochs).\n",
      "============================================================\n",
      "Fold 2/5\n",
      "Train subjects (n=26): ['07285e', '220a17', '24a59d', '2c98f7', '2d57c2', '31d269', '364459', '3b2403', '48fd62', '4b39ac', '4ca9b3', '516a67', '54ee6e', '66341b', '79011a', '7eb666', '7fcee9', '87174c', '8db7dd', '93f49f', 'a03db7', 'bc3908', 'c8e721', 'd8836b', 'd9312a', 'e8919c']\n",
      "Val subjects   (n=8): ['194d1d', '231c3b', '251738', '2a39f8', '4f13b4', '7688c1', 'c85fdf', 'f2c8aa']\n",
      "Train windows: 15880, Val windows: 4000\n",
      "[Epoch 01] Train Loss=0.5699, Acc: 0.7108, F1=0.5381 | Val Loss=0.8466, Acc: 0.5837 F1=0.0599\n",
      "[Epoch 02] Train Loss=0.4260, Acc: 0.8129, F1=0.7393 | Val Loss=0.6469, Acc: 0.6195 F1=0.2787\n",
      "[Epoch 03] Train Loss=0.3870, Acc: 0.8334, F1=0.7691 | Val Loss=0.7723, Acc: 0.6105 F1=0.2733\n",
      "[Epoch 04] Train Loss=0.3690, Acc: 0.8419, F1=0.7831 | Val Loss=0.9095, Acc: 0.6038 F1=0.1826\n",
      "[Epoch 05] Train Loss=0.3445, Acc: 0.8582, F1=0.8071 | Val Loss=0.9206, Acc: 0.6208 F1=0.2256\n",
      "[Epoch 06] Train Loss=0.3252, Acc: 0.8630, F1=0.8153 | Val Loss=0.3783, Acc: 0.8347 F1=0.7862\n",
      "[Epoch 07] Train Loss=0.3145, Acc: 0.8700, F1=0.8262 | Val Loss=0.5244, Acc: 0.7560 F1=0.6509\n",
      "[Epoch 08] Train Loss=0.3029, Acc: 0.8748, F1=0.8328 | Val Loss=0.4913, Acc: 0.7788 F1=0.6692\n",
      "[Epoch 09] Train Loss=0.2932, Acc: 0.8780, F1=0.8375 | Val Loss=0.6365, Acc: 0.7438 F1=0.5937\n",
      "[Epoch 10] Train Loss=0.2861, Acc: 0.8804, F1=0.8416 | Val Loss=0.3546, Acc: 0.8545 F1=0.8138\n",
      "[Epoch 11] Train Loss=0.2778, Acc: 0.8854, F1=0.8492 | Val Loss=0.4525, Acc: 0.8077 F1=0.7255\n",
      "[Epoch 12] Train Loss=0.2730, Acc: 0.8872, F1=0.8517 | Val Loss=0.3874, Acc: 0.8377 F1=0.8004\n",
      "[Epoch 13] Train Loss=0.2650, Acc: 0.8895, F1=0.8551 | Val Loss=0.5121, Acc: 0.7850 F1=0.6735\n",
      "[Epoch 14] Train Loss=0.2604, Acc: 0.8922, F1=0.8589 | Val Loss=0.3193, Acc: 0.8695 F1=0.8402\n",
      "[Epoch 15] Train Loss=0.2545, Acc: 0.8938, F1=0.8609 | Val Loss=0.4654, Acc: 0.8045 F1=0.7221\n",
      "[Epoch 16] Train Loss=0.2516, Acc: 0.8958, F1=0.8640 | Val Loss=0.4072, Acc: 0.8393 F1=0.7842\n",
      "[Epoch 17] Train Loss=0.2472, Acc: 0.8951, F1=0.8631 | Val Loss=0.4298, Acc: 0.8247 F1=0.7686\n",
      "[Epoch 18] Train Loss=0.2438, Acc: 0.9000, F1=0.8692 | Val Loss=0.4269, Acc: 0.8275 F1=0.7669\n",
      "[Epoch 19] Train Loss=0.2411, Acc: 0.9001, F1=0.8699 | Val Loss=0.4177, Acc: 0.8303 F1=0.7600\n",
      "[Epoch 20] Train Loss=0.2383, Acc: 0.9023, F1=0.8729 | Val Loss=0.4930, Acc: 0.7925 F1=0.7023\n",
      "[Epoch 21] Train Loss=0.2284, Acc: 0.9065, F1=0.8786 | Val Loss=0.3481, Acc: 0.8628 F1=0.8254\n",
      "Early stopping at epoch 21 (no val F1 improvement for 7 epochs).\n",
      "============================================================\n",
      "Fold 3/5\n",
      "Train subjects (n=26): ['194d1d', '220a17', '231c3b', '24a59d', '251738', '2a39f8', '2c98f7', '2d57c2', '31d269', '4b39ac', '4ca9b3', '4f13b4', '516a67', '7688c1', '79011a', '7eb666', '7fcee9', '87174c', '8db7dd', 'a03db7', 'bc3908', 'c85fdf', 'c8e721', 'd8836b', 'e8919c', 'f2c8aa']\n",
      "Val subjects   (n=8): ['07285e', '364459', '3b2403', '48fd62', '54ee6e', '66341b', '93f49f', 'd9312a']\n",
      "Train windows: 15909, Val windows: 3971\n",
      "[Epoch 01] Train Loss=0.6216, Acc: 0.6287, F1=0.5315 | Val Loss=0.7121, Acc: 0.5412 F1=0.3350\n",
      "[Epoch 02] Train Loss=0.5237, Acc: 0.7198, F1=0.6897 | Val Loss=0.6906, Acc: 0.5528 F1=0.3318\n",
      "[Epoch 03] Train Loss=0.4285, Acc: 0.7823, F1=0.7579 | Val Loss=0.4288, Acc: 0.8333 F1=0.2119\n",
      "[Epoch 04] Train Loss=0.3824, Acc: 0.8150, F1=0.7932 | Val Loss=0.4547, Acc: 0.8320 F1=0.2162\n",
      "[Epoch 05] Train Loss=0.3572, Acc: 0.8324, F1=0.8118 | Val Loss=0.5070, Acc: 0.7225 F1=0.2963\n",
      "[Epoch 06] Train Loss=0.3380, Acc: 0.8542, F1=0.8367 | Val Loss=0.4275, Acc: 0.8313 F1=0.3232\n",
      "[Epoch 07] Train Loss=0.3195, Acc: 0.8684, F1=0.8534 | Val Loss=0.4453, Acc: 0.8018 F1=0.3837\n",
      "[Epoch 08] Train Loss=0.3045, Acc: 0.8747, F1=0.8612 | Val Loss=0.4393, Acc: 0.8053 F1=0.3831\n",
      "[Epoch 09] Train Loss=0.2935, Acc: 0.8789, F1=0.8658 | Val Loss=0.4450, Acc: 0.8086 F1=0.4163\n",
      "[Epoch 10] Train Loss=0.2837, Acc: 0.8866, F1=0.8751 | Val Loss=0.4338, Acc: 0.8199 F1=0.3956\n",
      "[Epoch 11] Train Loss=0.2756, Acc: 0.8897, F1=0.8786 | Val Loss=0.4257, Acc: 0.8194 F1=0.4147\n",
      "[Epoch 12] Train Loss=0.2700, Acc: 0.8908, F1=0.8798 | Val Loss=0.4285, Acc: 0.8265 F1=0.4166\n",
      "[Epoch 13] Train Loss=0.2658, Acc: 0.8928, F1=0.8820 | Val Loss=0.4194, Acc: 0.8222 F1=0.3966\n",
      "[Epoch 14] Train Loss=0.2628, Acc: 0.8942, F1=0.8836 | Val Loss=0.4075, Acc: 0.8252 F1=0.4188\n",
      "[Epoch 15] Train Loss=0.2580, Acc: 0.8992, F1=0.8892 | Val Loss=0.4369, Acc: 0.8061 F1=0.4254\n",
      "[Epoch 16] Train Loss=0.2543, Acc: 0.9010, F1=0.8909 | Val Loss=0.4054, Acc: 0.8308 F1=0.3978\n",
      "[Epoch 17] Train Loss=0.2520, Acc: 0.9005, F1=0.8905 | Val Loss=0.4150, Acc: 0.8169 F1=0.4342\n",
      "[Epoch 18] Train Loss=0.2513, Acc: 0.8991, F1=0.8890 | Val Loss=0.4112, Acc: 0.8351 F1=0.3861\n",
      "[Epoch 19] Train Loss=0.2472, Acc: 0.9045, F1=0.8948 | Val Loss=0.4092, Acc: 0.8283 F1=0.4070\n",
      "[Epoch 20] Train Loss=0.2431, Acc: 0.9040, F1=0.8943 | Val Loss=0.4133, Acc: 0.8461 F1=0.3784\n",
      "[Epoch 21] Train Loss=0.2412, Acc: 0.9058, F1=0.8960 | Val Loss=0.4026, Acc: 0.8353 F1=0.4065\n",
      "[Epoch 22] Train Loss=0.2387, Acc: 0.9059, F1=0.8963 | Val Loss=0.4208, Acc: 0.8149 F1=0.4244\n",
      "[Epoch 23] Train Loss=0.2379, Acc: 0.9077, F1=0.8983 | Val Loss=0.4294, Acc: 0.8471 F1=0.3409\n",
      "[Epoch 24] Train Loss=0.2338, Acc: 0.9094, F1=0.9001 | Val Loss=0.4316, Acc: 0.8157 F1=0.4263\n",
      "Early stopping at epoch 24 (no val F1 improvement for 7 epochs).\n",
      "============================================================\n",
      "Fold 4/5\n",
      "Train subjects (n=26): ['07285e', '194d1d', '220a17', '231c3b', '251738', '2a39f8', '2c98f7', '2d57c2', '31d269', '364459', '3b2403', '48fd62', '4f13b4', '516a67', '54ee6e', '66341b', '7688c1', '7fcee9', '87174c', '93f49f', 'a03db7', 'bc3908', 'c85fdf', 'd9312a', 'e8919c', 'f2c8aa']\n",
      "Val subjects   (n=8): ['24a59d', '4b39ac', '4ca9b3', '79011a', '7eb666', '8db7dd', 'c8e721', 'd8836b']\n",
      "Train windows: 15916, Val windows: 3964\n",
      "[Epoch 01] Train Loss=0.6279, Acc: 0.6342, F1=0.4039 | Val Loss=0.5512, Acc: 0.7071 F1=0.2703\n",
      "[Epoch 02] Train Loss=0.4706, Acc: 0.7626, F1=0.6968 | Val Loss=0.4873, Acc: 0.7293 F1=0.6244\n",
      "[Epoch 03] Train Loss=0.4158, Acc: 0.7933, F1=0.7356 | Val Loss=0.5069, Acc: 0.7321 F1=0.6279\n",
      "[Epoch 04] Train Loss=0.3922, Acc: 0.8104, F1=0.7567 | Val Loss=0.4345, Acc: 0.8063 F1=0.6235\n",
      "[Epoch 05] Train Loss=0.3640, Acc: 0.8366, F1=0.7916 | Val Loss=0.4059, Acc: 0.8413 F1=0.7649\n",
      "[Epoch 06] Train Loss=0.3365, Acc: 0.8547, F1=0.8181 | Val Loss=0.3746, Acc: 0.8234 F1=0.6903\n",
      "[Epoch 07] Train Loss=0.3203, Acc: 0.8627, F1=0.8297 | Val Loss=0.3583, Acc: 0.8542 F1=0.7817\n",
      "[Epoch 08] Train Loss=0.3065, Acc: 0.8690, F1=0.8386 | Val Loss=0.3479, Acc: 0.8534 F1=0.7704\n",
      "[Epoch 09] Train Loss=0.3007, Acc: 0.8711, F1=0.8417 | Val Loss=0.3378, Acc: 0.8577 F1=0.7753\n",
      "[Epoch 10] Train Loss=0.2924, Acc: 0.8791, F1=0.8515 | Val Loss=0.3397, Acc: 0.8537 F1=0.7800\n",
      "[Epoch 11] Train Loss=0.2861, Acc: 0.8797, F1=0.8517 | Val Loss=0.3312, Acc: 0.8532 F1=0.7546\n",
      "[Epoch 12] Train Loss=0.2781, Acc: 0.8829, F1=0.8567 | Val Loss=0.3370, Acc: 0.8607 F1=0.7882\n",
      "[Epoch 13] Train Loss=0.2738, Acc: 0.8858, F1=0.8602 | Val Loss=0.3227, Acc: 0.8618 F1=0.7854\n",
      "[Epoch 14] Train Loss=0.2696, Acc: 0.8855, F1=0.8600 | Val Loss=0.3270, Acc: 0.8650 F1=0.7881\n",
      "[Epoch 15] Train Loss=0.2639, Acc: 0.8897, F1=0.8650 | Val Loss=0.3240, Acc: 0.8580 F1=0.7678\n",
      "[Epoch 16] Train Loss=0.2604, Acc: 0.8902, F1=0.8659 | Val Loss=0.3390, Acc: 0.8620 F1=0.7999\n",
      "[Epoch 17] Train Loss=0.2570, Acc: 0.8912, F1=0.8670 | Val Loss=0.3315, Acc: 0.8655 F1=0.8050\n",
      "[Epoch 18] Train Loss=0.2515, Acc: 0.8935, F1=0.8702 | Val Loss=0.3262, Acc: 0.8585 F1=0.7671\n",
      "[Epoch 19] Train Loss=0.2479, Acc: 0.8961, F1=0.8731 | Val Loss=0.3198, Acc: 0.8630 F1=0.7822\n",
      "[Epoch 20] Train Loss=0.2472, Acc: 0.8955, F1=0.8730 | Val Loss=0.3174, Acc: 0.8671 F1=0.7966\n",
      "[Epoch 21] Train Loss=0.2447, Acc: 0.8989, F1=0.8768 | Val Loss=0.3229, Acc: 0.8645 F1=0.7856\n",
      "[Epoch 22] Train Loss=0.2424, Acc: 0.8995, F1=0.8776 | Val Loss=0.3339, Acc: 0.8643 F1=0.8049\n",
      "[Epoch 23] Train Loss=0.2395, Acc: 0.8998, F1=0.8780 | Val Loss=0.3428, Acc: 0.8668 F1=0.8084\n",
      "[Epoch 24] Train Loss=0.2358, Acc: 0.9014, F1=0.8802 | Val Loss=0.3402, Acc: 0.8696 F1=0.8122\n",
      "[Epoch 25] Train Loss=0.2324, Acc: 0.9029, F1=0.8821 | Val Loss=0.3740, Acc: 0.8575 F1=0.8020\n",
      "[Epoch 26] Train Loss=0.2306, Acc: 0.9051, F1=0.8846 | Val Loss=0.3305, Acc: 0.8625 F1=0.7901\n",
      "[Epoch 27] Train Loss=0.2224, Acc: 0.9067, F1=0.8867 | Val Loss=0.3192, Acc: 0.8724 F1=0.8008\n",
      "[Epoch 28] Train Loss=0.2192, Acc: 0.9103, F1=0.8910 | Val Loss=0.3325, Acc: 0.8683 F1=0.8067\n",
      "[Epoch 29] Train Loss=0.2171, Acc: 0.9114, F1=0.8927 | Val Loss=0.3373, Acc: 0.8708 F1=0.8105\n",
      "[Epoch 30] Train Loss=0.2163, Acc: 0.9125, F1=0.8942 | Val Loss=0.3218, Acc: 0.8769 F1=0.8139\n",
      "[Epoch 31] Train Loss=0.2156, Acc: 0.9115, F1=0.8927 | Val Loss=0.3294, Acc: 0.8726 F1=0.8104\n",
      "[Epoch 32] Train Loss=0.2134, Acc: 0.9139, F1=0.8956 | Val Loss=0.3346, Acc: 0.8749 F1=0.8138\n",
      "[Epoch 33] Train Loss=0.2091, Acc: 0.9144, F1=0.8963 | Val Loss=0.3357, Acc: 0.8741 F1=0.8115\n",
      "[Epoch 34] Train Loss=0.2070, Acc: 0.9162, F1=0.8986 | Val Loss=0.3464, Acc: 0.8739 F1=0.8150\n",
      "[Epoch 35] Train Loss=0.2070, Acc: 0.9162, F1=0.8985 | Val Loss=0.3441, Acc: 0.8716 F1=0.8111\n",
      "[Epoch 36] Train Loss=0.2066, Acc: 0.9158, F1=0.8980 | Val Loss=0.3583, Acc: 0.8623 F1=0.8046\n",
      "[Epoch 37] Train Loss=0.2055, Acc: 0.9162, F1=0.8985 | Val Loss=0.3345, Acc: 0.8761 F1=0.8145\n",
      "[Epoch 38] Train Loss=0.2050, Acc: 0.9159, F1=0.8981 | Val Loss=0.3394, Acc: 0.8713 F1=0.8070\n",
      "[Epoch 39] Train Loss=0.2014, Acc: 0.9198, F1=0.9030 | Val Loss=0.3427, Acc: 0.8718 F1=0.8089\n",
      "[Epoch 40] Train Loss=0.2012, Acc: 0.9203, F1=0.9034 | Val Loss=0.3403, Acc: 0.8769 F1=0.8182\n",
      "[Epoch 41] Train Loss=0.2003, Acc: 0.9191, F1=0.9020 | Val Loss=0.3393, Acc: 0.8739 F1=0.8123\n",
      "[Epoch 42] Train Loss=0.2006, Acc: 0.9194, F1=0.9025 | Val Loss=0.3414, Acc: 0.8751 F1=0.8127\n",
      "[Epoch 43] Train Loss=0.2011, Acc: 0.9195, F1=0.9026 | Val Loss=0.3455, Acc: 0.8744 F1=0.8150\n",
      "[Epoch 44] Train Loss=0.1998, Acc: 0.9205, F1=0.9037 | Val Loss=0.3380, Acc: 0.8741 F1=0.8111\n",
      "[Epoch 45] Train Loss=0.1986, Acc: 0.9205, F1=0.9038 | Val Loss=0.3473, Acc: 0.8731 F1=0.8134\n",
      "[Epoch 46] Train Loss=0.1978, Acc: 0.9206, F1=0.9039 | Val Loss=0.3429, Acc: 0.8706 F1=0.8056\n",
      "[Epoch 47] Train Loss=0.1964, Acc: 0.9218, F1=0.9053 | Val Loss=0.3508, Acc: 0.8724 F1=0.8116\n",
      "Early stopping at epoch 47 (no val F1 improvement for 7 epochs).\n",
      "============================================================\n",
      "Fold 5/5\n",
      "Train subjects (n=26): ['07285e', '194d1d', '231c3b', '24a59d', '251738', '2a39f8', '2d57c2', '364459', '3b2403', '48fd62', '4b39ac', '4ca9b3', '4f13b4', '54ee6e', '66341b', '7688c1', '79011a', '7eb666', '87174c', '8db7dd', '93f49f', 'c85fdf', 'c8e721', 'd8836b', 'd9312a', 'f2c8aa']\n",
      "Val subjects   (n=8): ['220a17', '2c98f7', '31d269', '516a67', '7fcee9', 'a03db7', 'bc3908', 'e8919c']\n",
      "Train windows: 15919, Val windows: 3961\n",
      "[Epoch 01] Train Loss=0.6197, Acc: 0.6277, F1=0.5128 | Val Loss=0.5405, Acc: 0.7508 F1=0.4803\n",
      "[Epoch 02] Train Loss=0.4397, Acc: 0.7853, F1=0.7528 | Val Loss=0.5359, Acc: 0.7579 F1=0.5236\n",
      "[Epoch 03] Train Loss=0.3614, Acc: 0.8448, F1=0.8223 | Val Loss=0.4041, Acc: 0.8258 F1=0.5583\n",
      "[Epoch 04] Train Loss=0.3234, Acc: 0.8621, F1=0.8438 | Val Loss=0.4813, Acc: 0.7768 F1=0.5328\n",
      "[Epoch 05] Train Loss=0.3012, Acc: 0.8768, F1=0.8603 | Val Loss=0.4895, Acc: 0.7796 F1=0.5611\n",
      "[Epoch 06] Train Loss=0.2857, Acc: 0.8818, F1=0.8669 | Val Loss=0.4185, Acc: 0.8139 F1=0.5839\n",
      "[Epoch 07] Train Loss=0.2757, Acc: 0.8858, F1=0.8713 | Val Loss=0.4257, Acc: 0.8091 F1=0.5851\n",
      "[Epoch 08] Train Loss=0.2709, Acc: 0.8893, F1=0.8754 | Val Loss=0.4403, Acc: 0.8023 F1=0.5756\n",
      "[Epoch 09] Train Loss=0.2641, Acc: 0.8904, F1=0.8768 | Val Loss=0.3986, Acc: 0.8185 F1=0.5865\n",
      "[Epoch 10] Train Loss=0.2560, Acc: 0.8967, F1=0.8838 | Val Loss=0.4789, Acc: 0.7773 F1=0.5638\n",
      "[Epoch 11] Train Loss=0.2528, Acc: 0.8955, F1=0.8825 | Val Loss=0.4129, Acc: 0.8129 F1=0.5849\n",
      "[Epoch 12] Train Loss=0.2490, Acc: 0.8978, F1=0.8849 | Val Loss=0.4032, Acc: 0.8200 F1=0.5965\n",
      "[Epoch 13] Train Loss=0.2408, Acc: 0.9010, F1=0.8887 | Val Loss=0.3734, Acc: 0.8356 F1=0.5959\n",
      "[Epoch 14] Train Loss=0.2386, Acc: 0.9028, F1=0.8911 | Val Loss=0.3796, Acc: 0.8324 F1=0.6010\n",
      "[Epoch 15] Train Loss=0.2332, Acc: 0.9059, F1=0.8944 | Val Loss=0.4302, Acc: 0.8084 F1=0.5859\n",
      "[Epoch 16] Train Loss=0.2302, Acc: 0.9066, F1=0.8950 | Val Loss=0.3672, Acc: 0.8422 F1=0.6077\n",
      "[Epoch 17] Train Loss=0.2273, Acc: 0.9088, F1=0.8976 | Val Loss=0.3901, Acc: 0.8326 F1=0.6102\n",
      "[Epoch 18] Train Loss=0.2236, Acc: 0.9082, F1=0.8974 | Val Loss=0.3690, Acc: 0.8374 F1=0.6020\n",
      "[Epoch 19] Train Loss=0.2219, Acc: 0.9109, F1=0.8999 | Val Loss=0.3734, Acc: 0.8364 F1=0.5965\n",
      "[Epoch 20] Train Loss=0.2178, Acc: 0.9132, F1=0.9027 | Val Loss=0.3877, Acc: 0.8301 F1=0.6089\n",
      "[Epoch 21] Train Loss=0.2172, Acc: 0.9122, F1=0.9016 | Val Loss=0.4471, Acc: 0.8124 F1=0.5964\n",
      "[Epoch 22] Train Loss=0.2123, Acc: 0.9148, F1=0.9046 | Val Loss=0.3826, Acc: 0.8374 F1=0.6073\n",
      "[Epoch 23] Train Loss=0.2039, Acc: 0.9176, F1=0.9077 | Val Loss=0.3831, Acc: 0.8367 F1=0.5887\n",
      "[Epoch 24] Train Loss=0.2024, Acc: 0.9202, F1=0.9105 | Val Loss=0.4372, Acc: 0.8210 F1=0.6015\n",
      "Early stopping at epoch 24 (no val F1 improvement for 7 epochs).\n",
      "ðŸ“ Saved best model for Fold 0 â†’ all_best_models\\best_model_fold_0.pt\n",
      "ðŸ“ Saved best model for Fold 1 â†’ all_best_models\\best_model_fold_1.pt\n",
      "ðŸ“ Saved best model for Fold 2 â†’ all_best_models\\best_model_fold_2.pt\n",
      "ðŸ“ Saved best model for Fold 3 â†’ all_best_models\\best_model_fold_3.pt\n",
      "ðŸ“ Saved best model for Fold 4 â†’ all_best_models\\best_model_fold_4.pt\n",
      "âœ… Fold metrics saved to Excel: fog_cv_folds.xlsx\n",
      "âœ… Summary metrics saved to Excel: fog_cv_summary.xlsx\n",
      "ðŸ† Global best model (Fold 0) saved â†’ best_fog_model.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cv_results = cross_validate_patient_independent(\n",
    "    train_df,\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    input_dim=3,\n",
    "    hidden_dim=64,\n",
    "    num_layers=1,\n",
    "    bidirectional=False,\n",
    "    dropout=0.2,\n",
    "    batch_size=16,\n",
    "    num_epochs=50,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    early_stopping_patience=7,\n",
    "    device=None,  # auto: cuda if available else cpu\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e820ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, average_precision_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_models_on_test_ensemble(\n",
    "    test_df: pd.DataFrame,\n",
    "    model_paths: List[str],\n",
    "    batch_size: int = 32,\n",
    "    device: str = None,\n",
    "    fold_weights: List[float] = None   # OPTIONAL for weighted voting\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate an ensemble of models on the test set using:\n",
    "        - Soft voting (default)\n",
    "        - Hard majority voting\n",
    "        - Optional weighted voting\n",
    "\n",
    "    Args:\n",
    "        test_df        : Test dataframe\n",
    "        model_paths    : List of paths to saved fold models\n",
    "        batch_size     : Test batch size\n",
    "        device         : 'cpu' or 'cuda'\n",
    "        fold_weights   : Optional weights per fold (e.g. fold F1)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with all ensemble metrics and voting predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Device Setup\n",
    "    # -----------------------------\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # -----------------------------\n",
    "    # DataLoader\n",
    "    # -----------------------------\n",
    "    test_loader = make_dataloader(test_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # collect predictions from each model\n",
    "    prob_list = []     # soft voting\n",
    "    hard_list = []     # hard voting\n",
    "    targets_list = []\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load each model and predict\n",
    "    # -----------------------------\n",
    "    for idx, model_path in enumerate(model_paths):\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "\n",
    "        print(f\"ðŸ“¥ Loading model: {model_path}\")\n",
    "\n",
    "        model = LSTMTransformer().to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        fold_probs = []\n",
    "        fold_hard = []\n",
    "        fold_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                logits = model(X)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "                fold_probs.extend(probs)\n",
    "                fold_hard.extend(preds)\n",
    "                fold_targets.extend(y.cpu().numpy().astype(int))\n",
    "\n",
    "        prob_list.append(np.array(fold_probs))\n",
    "        hard_list.append(np.array(fold_hard))\n",
    "        targets_list = fold_targets     # same for all folds\n",
    "\n",
    "    prob_matrix = np.vstack(prob_list)     # shape: (num_models, N)\n",
    "    hard_matrix = np.vstack(hard_list)     # shape: (num_models, N)\n",
    "    ground_truth = np.array(targets_list)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Voting Methods\n",
    "    # -----------------------------\n",
    "\n",
    "    # SOFT VOTING (best default)\n",
    "    soft_probs = prob_matrix.mean(axis=0)\n",
    "    soft_preds = (soft_probs >= 0.5).astype(int)\n",
    "\n",
    "    # HARD VOTING\n",
    "    hard_preds = np.round(hard_matrix.mean(axis=0)).astype(int)\n",
    "\n",
    "    # WEIGHTED VOTING (if provided)\n",
    "    if fold_weights is not None:\n",
    "        w = np.array(fold_weights).reshape(-1, 1)\n",
    "        weighted_probs = (prob_matrix * w).sum(axis=0) / w.sum()\n",
    "        weighted_preds = (weighted_probs >= 0.5).astype(int)\n",
    "    else:\n",
    "        weighted_preds = None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Metric Function\n",
    "    # -----------------------------\n",
    "    def compute_metrics(preds, probs=None):\n",
    "        return {\n",
    "            \"loss\": criterion(\n",
    "                torch.tensor(preds, dtype=torch.float32),\n",
    "                torch.tensor(ground_truth, dtype=torch.float32)\n",
    "            ).item(),\n",
    "            \"accuracy\": accuracy_score(ground_truth, preds),\n",
    "            \"precision\": precision_score(ground_truth, preds, zero_division=0),\n",
    "            \"recall\": recall_score(ground_truth, preds, zero_division=0),\n",
    "            \"f1\": f1_score(ground_truth, preds, zero_division=0),\n",
    "            \"roc_auc\": roc_auc_score(ground_truth, probs) if probs is not None else None,\n",
    "            \"pr_auc\": average_precision_score(ground_truth, probs) if probs is not None else None,\n",
    "            \"confusion_matrix\": confusion_matrix(ground_truth, preds)\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Compute Metrics\n",
    "    # -----------------------------\n",
    "    metrics_soft = compute_metrics(soft_preds, soft_probs)\n",
    "    metrics_hard = compute_metrics(hard_preds, soft_probs)\n",
    "    metrics_weighted = compute_metrics(weighted_preds, weighted_probs) if weighted_preds is not None else None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Print Results\n",
    "    # -----------------------------\n",
    "    print(\"\\n\\n **SOFT VOTING RESULTS**\")\n",
    "    for k, v in metrics_soft.items():\n",
    "        if k != \"confusion_matrix\":\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    print(\"\\n **HARD VOTING RESULTS**\")\n",
    "    for k, v in metrics_hard.items():\n",
    "        if k != \"confusion_matrix\":\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    if metrics_weighted is not None:\n",
    "        print(\"\\n **WEIGHTED VOTING RESULTS**\")\n",
    "        for k, v in metrics_weighted.items():\n",
    "            if k != \"confusion_matrix\":\n",
    "                print(f\"{k}: {v}\")\n",
    "\n",
    "    return {\n",
    "        \"metrics_soft\": metrics_soft,\n",
    "        \"metrics_hard\": metrics_hard,\n",
    "        \"metrics_weighted\": metrics_weighted,\n",
    "        \"soft_preds\": soft_preds,\n",
    "        \"soft_probs\": soft_probs,\n",
    "        \"hard_preds\": hard_preds,\n",
    "        \"weighted_preds\": weighted_preds\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828f2b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\all_best_models\\\\best_model_fold_0.pt',\n",
       " '.\\\\all_best_models\\\\best_model_fold_1.pt',\n",
       " '.\\\\all_best_models\\\\best_model_fold_2.pt',\n",
       " '.\\\\all_best_models\\\\best_model_fold_3.pt',\n",
       " '.\\\\all_best_models\\\\best_model_fold_4.pt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "directory_path = '.\\\\all_best_models\\\\'\n",
    "file_paths = []\n",
    "for root, _, files in os.walk(directory_path):\n",
    "    for file in files:\n",
    "        file_paths.append(os.path.join(root, file))\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e30d0a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading model: .\\all_best_models\\best_model_fold_0.pt\n",
      "ðŸ“¥ Loading model: .\\all_best_models\\best_model_fold_1.pt\n",
      "ðŸ“¥ Loading model: .\\all_best_models\\best_model_fold_2.pt\n",
      "ðŸ“¥ Loading model: .\\all_best_models\\best_model_fold_3.pt\n",
      "ðŸ“¥ Loading model: .\\all_best_models\\best_model_fold_4.pt\n",
      "\n",
      "\n",
      "ðŸŽ¯ **SOFT VOTING RESULTS**\n",
      "loss: 0.6947153806686401\n",
      "accuracy: 0.897876643073812\n",
      "precision: 0.6115702479338843\n",
      "recall: 0.784452296819788\n",
      "f1: 0.6873065015479877\n",
      "roc_auc: 0.9226617467713185\n",
      "pr_auc: 0.7164898461419864\n",
      "\n",
      "ðŸ—³ï¸ **HARD VOTING RESULTS**\n",
      "loss: 0.6997313499450684\n",
      "accuracy: 0.8897876643073812\n",
      "precision: 0.5857519788918206\n",
      "recall: 0.784452296819788\n",
      "f1: 0.6706948640483383\n",
      "roc_auc: 0.9226617467713185\n",
      "pr_auc: 0.7164898461419864\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_models_on_test_ensemble(\n",
    "    test_df=test_df,\n",
    "    model_paths= file_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71094667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ—³ï¸ **HARD VOTING RESULTS**\n",
    "# loss: 0.7022083401679993\n",
    "# accuracy: 0.8842264914054601\n",
    "# precision: 0.5721925133689839\n",
    "# recall: 0.7561837455830389\n",
    "# f1: 0.6514459665144596\n",
    "# roc_auc: 0.9207417367647519\n",
    "# pr_auc: 0.708372352847281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c584483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve\n",
    "\n",
    "\n",
    "def save_and_plot_ensemble_results(\n",
    "    eval_results: Dict[str, Any],\n",
    "    ground_truth: np.ndarray,\n",
    "    output_folder: str = \"ensemble_results/\",\n",
    "    model_output_path: str = \"ensemble_final_model.pt\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates plots (confusion matrix, ROC, PR curve),\n",
    "    saves predictions, and exports the ensemble model.\n",
    "\n",
    "    Args:\n",
    "        eval_results      : Output of evaluate_models_on_test_ensemble()\n",
    "        ground_truth      : Numpy array of true labels\n",
    "        output_folder     : Directory to save images & CSV\n",
    "        model_output_path : File to save final ensemble soft-voting model weights\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract predictions\n",
    "    # -----------------------------\n",
    "    soft_probs = eval_results[\"soft_probs\"]\n",
    "    soft_preds = eval_results[\"soft_preds\"]\n",
    "    hard_preds = eval_results[\"hard_preds\"]\n",
    "    weighted_preds = eval_results[\"weighted_preds\"]\n",
    "\n",
    "    # =============================\n",
    "    #  1. Save predictions to CSV\n",
    "    # =============================\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"y_true\": ground_truth,\n",
    "        \"soft_prob\": soft_probs,\n",
    "        \"soft_pred\": soft_preds,\n",
    "        \"hard_pred\": hard_preds,\n",
    "        \"weighted_pred\": weighted_preds if weighted_preds is not None else np.nan\n",
    "    })\n",
    "\n",
    "    csv_path = os.path.join(output_folder, \"ensemble_predictions.csv\")\n",
    "    pred_df.to_csv(csv_path, index=False)\n",
    "    print(f\" Predictions saved to: {csv_path}\")\n",
    "\n",
    "    # =============================\n",
    "    #  2. Confusion Matrix Plot\n",
    "    # =============================\n",
    "    cm = confusion_matrix(ground_truth, soft_preds)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(cm, cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    labels = [\" Non FoG\", \"FoG\"]\n",
    "\n",
    "    plt.xticks(ticks=[0, 1], labels=labels)\n",
    "    plt.yticks(ticks=[0, 1], labels=labels)\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, str(cm[i, j]), ha='center', va='center', color='red')\n",
    "\n",
    "    cm_path = os.path.join(output_folder, \"confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\" Confusion Matrix saved to: {cm_path}\")\n",
    "\n",
    "    # =============================\n",
    "    #  3. ROC Curve Plot\n",
    "    # =============================\n",
    "    fpr, tpr, _ = roc_curve(ground_truth, soft_probs)\n",
    "    plt.figure(figsize=(6, 8))\n",
    "    plt.plot(fpr, tpr, label=\"ROC\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Soft Voting)\")\n",
    "    plt.legend()\n",
    "\n",
    "    roc_path = os.path.join(output_folder, \"roc_curve.png\")\n",
    "    plt.savefig(roc_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\" ROC Curve saved to: {roc_path}\")\n",
    "\n",
    "    # =============================\n",
    "    #  4. Precisionâ€“Recall Curve\n",
    "    # =============================\n",
    "    precision, recall, _ = precision_recall_curve(ground_truth, soft_probs)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(recall, precision, label=\"Precisionâ€“Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precisionâ€“Recall Curve (Soft Voting)\")\n",
    "    plt.legend()\n",
    "\n",
    "    pr_path = os.path.join(output_folder, \"pr_curve.png\")\n",
    "    plt.savefig(pr_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\" Precisionâ€“Recall Curve saved to: {pr_path}\")\n",
    "\n",
    "    # =============================\n",
    "    #  5. Export Final Ensemble Model\n",
    "    # =============================\n",
    "\n",
    "    \"\"\"\n",
    "    Ensemble model: soft-voting means averaging probabilities.\n",
    "    You cannot save a single PyTorch state dict unless we create\n",
    "    a small wrapper module below.\n",
    "    \"\"\"\n",
    "\n",
    "    class SoftVotingEnsemble(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, prob_list):\n",
    "            \"\"\"\n",
    "            prob_list: tensor shape (num_models, batch_size)\n",
    "            \"\"\"\n",
    "            return prob_list.mean(dim=0)\n",
    "\n",
    "    ensemble_model = SoftVotingEnsemble()\n",
    "    torch.save(ensemble_model.state_dict(), model_output_path)\n",
    "\n",
    "    print(f\" Final Ensemble Model saved to: {model_output_path}\")\n",
    "\n",
    "    print(\"\\n ALL RESULTS SAVED SUCCESSFULLY!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b793e153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Predictions saved to: ensemble_results/ensemble_predictions.csv\n",
      "ðŸ“Š Confusion Matrix saved to: ensemble_results/confusion_matrix.png\n",
      "ðŸ“ˆ ROC Curve saved to: ensemble_results/roc_curve.png\n",
      "ðŸ“‰ Precisionâ€“Recall Curve saved to: ensemble_results/pr_curve.png\n",
      "ðŸ§  Final Ensemble Model saved to: ensemble_final_soft_voting.pt\n",
      "\n",
      "ðŸŽ‰ ALL RESULTS SAVED SUCCESSFULLY!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ground_truth = test_df['window_label'].values  # or however your labels stored\n",
    "\n",
    "save_and_plot_ensemble_results(\n",
    "    eval_results=results,\n",
    "    ground_truth=ground_truth,\n",
    "    output_folder=\"ensemble_results/\",\n",
    "    model_output_path=\"ensemble_final_soft_voting.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b285025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae9273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
