{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f488a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset & DataLoader helpers\n",
    "# -----------------------------\n",
    "\n",
    "class FoGWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for window-level FoG data.\n",
    "\n",
    "    Expects a DataFrame with columns:\n",
    "        - 'sequence'     : (T, 3) accel window (list/array/string)\n",
    "        - 'window_label' : 0/1\n",
    "        - 'subject'      : patient ID (not used in __getitem__, but kept in df)\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def _parse_sequence(self, seq_obj):\n",
    "        # Handles numpy array, python list, or string from CSV\n",
    "        if isinstance(seq_obj, np.ndarray):\n",
    "            arr = seq_obj.astype(np.float32)\n",
    "        elif isinstance(seq_obj, list):\n",
    "            arr = np.asarray(seq_obj, dtype=np.float32)\n",
    "        else:\n",
    "            # assume string like \"[[...], [...], ...]\"\n",
    "            arr = np.array(ast.literal_eval(seq_obj), dtype=np.float32)\n",
    "\n",
    "        # Ensure shape (T, 3)\n",
    "        if arr.ndim == 1:\n",
    "            arr = arr.reshape(-1, 1)\n",
    "        return arr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq = self._parse_sequence(row['sequence'])  # (T, 3)\n",
    "        label = float(row['window_label'])\n",
    "\n",
    "        x = torch.from_numpy(seq)               # (T, 3)\n",
    "        y = torch.tensor(label, dtype=torch.float32)  # scalar 0/1\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def make_dataloader(df: pd.DataFrame,\n",
    "                    batch_size: int = 64,\n",
    "                    shuffle: bool = True,\n",
    "                    num_workers: int = 0) -> DataLoader:\n",
    "    dataset = FoGWindowDataset(df)\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        num_workers=num_workers)\n",
    "    return loader\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Patient-independent folds\n",
    "# -----------------------------\n",
    "\n",
    "def create_group_kfold_splits(df: pd.DataFrame,\n",
    "                              n_splits: int = 5,\n",
    "                              random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Patient-independent K-fold splits using GroupKFold on 'subject'.\n",
    "\n",
    "    Returns a list of (train_idx, val_idx) index arrays.\n",
    "    \"\"\"\n",
    "    groups = df['subject'].values\n",
    "    y = df['window_label'].values\n",
    "    X = np.arange(len(df))\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    # GroupKFold is deterministic; we can shuffle subjects before if needed:\n",
    "    # but simplest is to just use GroupKFold directly.\n",
    "    splits = list(gkf.split(X, y, groups))\n",
    "    return splits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics helper\n",
    "# -----------------------------\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray,\n",
    "                    y_pred_probs: np.ndarray,\n",
    "                    threshold: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, recall, F1 for binary classification.\n",
    "    \"\"\"\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='binary', zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'accuracy': float(acc),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1': float(f1),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# One epoch train / eval\n",
    "# -----------------------------\n",
    "\n",
    "def run_epoch(model: nn.Module,\n",
    "              loader: DataLoader,\n",
    "              criterion,\n",
    "              device: torch.device,\n",
    "              optimizer=None) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    If optimizer is provided: training mode.\n",
    "    Otherwise: evaluation mode.\n",
    "\n",
    "    Returns dict: loss, accuracy, precision, recall, f1\n",
    "    \"\"\"\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "    else:\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "    all_losses = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)              # (batch, T, 3)\n",
    "        y = y.to(device)              # (batch,)\n",
    "\n",
    "        logits = model(x)             # (batch,)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        all_probs.append(probs.detach().cpu().numpy())\n",
    "        all_labels.append(y.detach().cpu().numpy())\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    metrics = compute_metrics(all_labels, all_probs)\n",
    "    metrics['loss'] = float(np.mean(all_losses))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training one fold (with early stopping & scheduler)\n",
    "# -----------------------------\n",
    "\n",
    "def train_one_fold(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    input_dim: int = 3,\n",
    "    hidden_dim: int = 128,\n",
    "    num_layers: int = 2,\n",
    "    bidirectional: bool = False,\n",
    "    dropout: float = 0.0,\n",
    "    batch_size: int = 16,\n",
    "    num_epochs: int = 50,\n",
    "    lr: float = 1e-4,\n",
    "    weight_decay: float = 1e-4,\n",
    "    early_stopping_patience: int = 7,\n",
    "    device: str = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train LSTM on one fold with early stopping on val F1 and\n",
    "    ReduceLROnPlateau scheduler on val loss.\n",
    "\n",
    "    Returns:\n",
    "        dict with:\n",
    "            - 'best_state_dict'\n",
    "            - 'history' : list of per-epoch metrics\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = make_dataloader(train_df, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = make_dataloader(val_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model, loss, optimizer, scheduler\n",
    "    #model =  ParallelCNNLSTMTransformer().to(device)\n",
    "    # ---------------------------\n",
    "    # Quick test\n",
    "    # ---------------------------\n",
    "\n",
    "    B = 4\n",
    "    T = 256\n",
    "    C = 3\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = TCN().to(device)\n",
    "\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    #criterion = WeightedBCEWithLogitsLoss()\n",
    "    #criterion = FocalTverskyLoss(gamma=0.75, alpha=0.7)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    best_val_f1 = -np.inf\n",
    "    best_state_dict = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    history = []  # list of dicts per epoch\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # --- Train ---\n",
    "        train_metrics = run_epoch(model, train_loader, criterion, device, optimizer)\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_metrics = run_epoch(model, val_loader, criterion, device, optimizer=None)\n",
    "\n",
    "        # Step scheduler on validation loss\n",
    "        scheduler.step(val_metrics['loss'])\n",
    "\n",
    "        # Early stopping on val F1\n",
    "        val_f1 = val_metrics['f1']\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Record metrics\n",
    "        epoch_record = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_metrics['loss'],\n",
    "            'train_accuracy': train_metrics['accuracy'],\n",
    "            'train_precision': train_metrics['precision'],\n",
    "            'train_recall': train_metrics['recall'],\n",
    "            'train_f1': train_metrics['f1'],\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_accuracy': val_metrics['accuracy'],\n",
    "            'val_precision': val_metrics['precision'],\n",
    "            'val_recall': val_metrics['recall'],\n",
    "            'val_f1': val_metrics['f1'],\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "        history.append(epoch_record)\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch:02d}] \"\n",
    "            f\"Train Loss={train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, F1={train_metrics['f1']:.4f} | \"\n",
    "\n",
    "            f\"Val Loss={val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f} F1={val_metrics['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no val F1 improvement for \"\n",
    "                  f\"{early_stopping_patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'best_state_dict': best_state_dict,\n",
    "        'history': history,\n",
    "    }\n",
    "\n",
    "def summarize_and_save_cv_results(\n",
    "        results: Dict[str, Any],\n",
    "        output_prefix: str = \"cv_results\",\n",
    "        save_best_model_path: str = \"best_model_overall.pt\",\n",
    "        save_all_folds_dir: str = \"all_best_models\"):\n",
    "    \"\"\"\n",
    "    - Extract best metrics from each fold\n",
    "    - Compute mean and std\n",
    "    - Save fold-wise results and summary to Excel\n",
    "    - Save ALL best model weights for all folds\n",
    "    - Save the global best model across folds\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    os.makedirs(save_all_folds_dir, exist_ok=True)\n",
    "\n",
    "    fold_best_rows = []\n",
    "    global_best_f1 = -np.inf\n",
    "    global_best_state_dict = None\n",
    "    global_best_fold = None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract best epoch per fold\n",
    "    # -----------------------------\n",
    "    for fold_data in results['folds']:\n",
    "        fold_idx = fold_data['fold_idx']\n",
    "        history = fold_data['history']\n",
    "\n",
    "        # Best epoch based on val_f1\n",
    "        best_epoch_record = max(history, key=lambda x: x['val_f1'])\n",
    "\n",
    "        row = {\n",
    "            'fold': fold_idx,\n",
    "            'val_accuracy': best_epoch_record['val_accuracy'],\n",
    "            'val_precision': best_epoch_record['val_precision'],\n",
    "            'val_recall': best_epoch_record['val_recall'],\n",
    "            'val_f1': best_epoch_record['val_f1'],\n",
    "            'val_loss': best_epoch_record['val_loss'],\n",
    "        }\n",
    "\n",
    "        fold_best_rows.append(row)\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # Save THIS FOLD'S best model separately\n",
    "        # ---------------------------------------\n",
    "        fold_model_path = os.path.join(\n",
    "            save_all_folds_dir, f\"best_model_fold_{fold_idx}.pt\"\n",
    "        )\n",
    "        torch.save(fold_data['best_state_dict'], fold_model_path)\n",
    "        print(f\" Saved best model for Fold {fold_idx} ‚Üí {fold_model_path}\")\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # Track the GLOBAL best model\n",
    "        # ---------------------------------------\n",
    "        if best_epoch_record['val_f1'] > global_best_f1:\n",
    "            global_best_f1 = best_epoch_record['val_f1']\n",
    "            global_best_state_dict = fold_data['best_state_dict']\n",
    "            global_best_fold = fold_idx\n",
    "\n",
    "    df_folds = pd.DataFrame(fold_best_rows)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Mean & Std\n",
    "    # -----------------------------\n",
    "    summary = {}\n",
    "    for col in ['val_accuracy', 'val_precision', 'val_recall', 'val_f1', 'val_loss']:\n",
    "        summary[f'{col}_mean'] = df_folds[col].mean()\n",
    "        summary[f'{col}_std'] = df_folds[col].std()\n",
    "\n",
    "    df_summary = pd.DataFrame([summary])\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save Excel files\n",
    "    # -----------------------------\n",
    "    folds_excel = f\"{output_prefix}_folds.xlsx\"\n",
    "    summary_excel = f\"{output_prefix}_summary.xlsx\"\n",
    "\n",
    "    df_folds.to_excel(folds_excel, index=False)\n",
    "    df_summary.to_excel(summary_excel, index=False)\n",
    "\n",
    "    print(f\" Fold metrics saved to Excel: {folds_excel}\")\n",
    "    print(f\" Summary metrics saved to Excel: {summary_excel}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save global best model\n",
    "    # -----------------------------\n",
    "    if global_best_state_dict is not None:\n",
    "        torch.save(global_best_state_dict, save_best_model_path)\n",
    "        print(f\" Global best model (Fold {global_best_fold}) saved ‚Üí {save_best_model_path}\")\n",
    "\n",
    "    return {\n",
    "        'fold_metrics_df': df_folds,\n",
    "        'summary_df': df_summary,\n",
    "        'best_fold': global_best_fold,\n",
    "        'best_f1': global_best_f1\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Full K-fold cross-validation\n",
    "# -----------------------------\n",
    "\n",
    "def cross_validate_patient_independent(\n",
    "    df: pd.DataFrame,\n",
    "    n_splits: int = 5,\n",
    "    random_state: int = 42,\n",
    "    **train_kwargs\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run patient-independent K-fold cross-validation.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          'folds': [\n",
    "            {\n",
    "              'fold_idx': 0,\n",
    "              'train_subjects': [...],\n",
    "              'val_subjects': [...],\n",
    "              'history': [...],           # list of per-epoch dicts\n",
    "              'best_state_dict': {...},\n",
    "            },\n",
    "            ...\n",
    "          ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    splits = create_group_kfold_splits(df, n_splits=n_splits, random_state=random_state)\n",
    "\n",
    "    results = {'folds': []}\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(splits):\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_subjects = sorted(train_df['subject'].unique())\n",
    "        val_subjects = sorted(val_df['subject'].unique())\n",
    "\n",
    "        print(f\"Train subjects (n={len(train_subjects)}): {train_subjects}\")\n",
    "        print(f\"Val subjects   (n={len(val_subjects)}): {val_subjects}\")\n",
    "        print(f\"Train windows: {len(train_df)}, Val windows: {len(val_df)}\")\n",
    "\n",
    "        fold_result = train_one_fold(\n",
    "            train_df=train_df,\n",
    "            val_df=val_df,\n",
    "            **train_kwargs\n",
    "        )\n",
    "\n",
    "        results['folds'].append({\n",
    "            'fold_idx': fold_idx,\n",
    "            'train_subjects': train_subjects,\n",
    "            'val_subjects': val_subjects,\n",
    "            'history': fold_result['history'],\n",
    "            'best_state_dict': fold_result['best_state_dict'],\n",
    "        })\n",
    "    #save the results:\n",
    "    summary = summarize_and_save_cv_results(\n",
    "        results,\n",
    "        output_prefix=\"fog_cv\",\n",
    "        save_best_model_path=\"best_fog_model.pt\"\n",
    "    )\n",
    "\n",
    "    results['summary'] = summary\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "127ecf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# --- AttentionLayer (fixed: no ReLU on scores) ---\n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Attention pooling over time.\n",
    "    Input: x (B, T, hidden_dim)\n",
    "    Output: context vector (B, hidden_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, hidden_dim)\n",
    "        scores = self.attention(x).squeeze(-1)      # (B, T)\n",
    "        weights = F.softmax(scores, dim=1)          # (B, T)\n",
    "        context = torch.sum(x * weights.unsqueeze(-1), dim=1)  # (B, hidden_dim)\n",
    "        return context\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# TCN building blocks (causal)\n",
    "# ---------------------------\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal TemporalBlock:\n",
    "    - Conv1d with dilation and left padding so output length == input length.\n",
    "    - Residual connection from input -> output (1x1 conv if channels differ).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "        # We'll pad manually in forward to achieve left-only (causal) padding\n",
    "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.downsample = None\n",
    "        if in_ch != out_ch:\n",
    "            self.downsample = nn.Conv1d(in_ch, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "        pad_left = (self.kernel_size - 1) * self.dilation\n",
    "        x_padded = F.pad(x, (pad_left, 0))         # pad only on left: (left, right)\n",
    "        out = self.conv1(x_padded)                 # (B, out_ch, T') where T' >= T\n",
    "        # Trim rightmost extra if any (should produce T or slightly > T depending on conv internals)\n",
    "        if out.shape[-1] > x.shape[-1]:\n",
    "            out = out[:, :, -x.shape[-1]:]\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        res = x\n",
    "        if self.downsample is not None:\n",
    "            res = self.downsample(x)\n",
    "\n",
    "        # If temporal lengths mismatch, align by trimming or padding (rare)\n",
    "        if res.shape[-1] != out.shape[-1]:\n",
    "            res = res[:, :, -out.shape[-1]:]\n",
    "\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TCNBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of TemporalBlocks with exponentially increasing dilation factors.\n",
    "    Input: (B, C, T)\n",
    "    Output: (B, T, feat_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, channels=[64, 64, 128, 128], kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        cur = in_channels\n",
    "        for i, ch in enumerate(channels):\n",
    "            dilation = 2 ** i\n",
    "            layers.append(TemporalBlock(cur, ch, kernel_size=kernel_size, dilation=dilation, dropout=dropout))\n",
    "            cur = ch\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.out_channels = cur\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "        y = self.net(x)                    # (B, out_ch, T)\n",
    "        y = y.permute(0, 2, 1).contiguous()  # -> (B, T, out_ch)\n",
    "        return y\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 seq_len=256,\n",
    "                 in_channels=3,\n",
    "                 tcn_channels=[32, 64, 128, 128],\n",
    "                 tcn_kernel=3,\n",
    "                 dropout=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        # TCN branch expects (B, C, T)\n",
    "        self.tcn = TCNBranch(in_channels=in_channels, channels=tcn_channels,\n",
    "                             kernel_size=tcn_kernel, dropout=dropout)\n",
    "\n",
    "        # Attention pooling must match tcn.out_channels\n",
    "        self.attention = AttentionLayer(hidden_dim=self.tcn.out_channels)\n",
    "\n",
    "        # Final classifier: first linear input uses tcn.out_channels\n",
    "        self.pool_dropout = nn.Dropout(0.2)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.tcn.out_channels, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)  <-- keep channels-first for conv1d operations\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Expected 3D input (B,C,T), got {x.shape}\")\n",
    "        x  = x.permute(0, 2, 1)\n",
    "        t = self.tcn(x)            # (B, T, feat_tcn)\n",
    "        # t is (B, T, feat) and matches AttentionLayer expectation\n",
    "        #t = t.permute(0, 2, 1)\n",
    "        #print(t.shape)\n",
    "        context = self.attention(t)   # (B, feat_tcn)\n",
    "        context = self.pool_dropout(context)\n",
    "\n",
    "        out = self.mlp(context).squeeze(-1)   # (B,)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c179b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Student\\\\Desktop\\\\Abouhashem\\\\DeepLearningProject\\\\'\n",
    "train_df = pd.read_pickle(path+ \"FoG_windows_train.pkl\")\n",
    "test_df  = pd.read_pickle(path+\"FoG_windows_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17cc619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Fold 1/5\n",
      "Train subjects (n=32): ['07285e', '194d1d', '220a17', '231c3b', '24a59d', '251738', '2a39f8', '2c98f7', '31d269', '364459', '3b2403', '48fd62', '4b39ac', '4ca9b3', '4f13b4', '516a67', '54ee6e', '66341b', '7688c1', '79011a', '7eb666', '7fcee9', '8db7dd', '93f49f', 'a03db7', 'bc3908', 'c85fdf', 'c8e721', 'd8836b', 'd9312a', 'e8919c', 'f2c8aa']\n",
      "Val subjects   (n=2): ['2d57c2', '87174c']\n",
      "Train windows: 15896, Val windows: 3984\n",
      "[Epoch 01] Train Loss=0.4655, Acc: 0.7671, F1=0.3478 | Val Loss=0.4816, Acc: 0.8155 F1=0.8870\n",
      "[Epoch 02] Train Loss=0.3643, Acc: 0.8376, F1=0.6686 | Val Loss=0.6794, Acc: 0.7166 F1=0.8140\n",
      "[Epoch 03] Train Loss=0.3336, Acc: 0.8554, F1=0.7119 | Val Loss=0.5207, Acc: 0.7620 F1=0.8482\n",
      "[Epoch 04] Train Loss=0.3088, Acc: 0.8692, F1=0.7423 | Val Loss=0.6564, Acc: 0.7236 F1=0.8195\n",
      "[Epoch 05] Train Loss=0.2887, Acc: 0.8807, F1=0.7686 | Val Loss=0.8109, Acc: 0.6867 F1=0.7901\n",
      "[Epoch 06] Train Loss=0.2749, Acc: 0.8845, F1=0.7756 | Val Loss=1.0672, Acc: 0.6119 F1=0.7254\n",
      "[Epoch 07] Train Loss=0.2645, Acc: 0.8885, F1=0.7837 | Val Loss=0.7098, Acc: 0.7508 F1=0.8407\n",
      "[Epoch 08] Train Loss=0.2505, Acc: 0.8957, F1=0.7993 | Val Loss=0.6381, Acc: 0.7593 F1=0.8466\n",
      "Early stopping at epoch 8 (no val F1 improvement for 7 epochs).\n",
      "============================================================\n",
      "Fold 2/5\n",
      "Train subjects (n=26): ['07285e', '220a17', '24a59d', '2c98f7', '2d57c2', '31d269', '364459', '3b2403', '48fd62', '4b39ac', '4ca9b3', '516a67', '54ee6e', '66341b', '79011a', '7eb666', '7fcee9', '87174c', '8db7dd', '93f49f', 'a03db7', 'bc3908', 'c8e721', 'd8836b', 'd9312a', 'e8919c']\n",
      "Val subjects   (n=8): ['194d1d', '231c3b', '251738', '2a39f8', '4f13b4', '7688c1', 'c85fdf', 'f2c8aa']\n",
      "Train windows: 15880, Val windows: 4000\n",
      "[Epoch 01] Train Loss=0.4417, Acc: 0.7950, F1=0.6949 | Val Loss=0.7545, Acc: 0.6280 F1=0.2819\n",
      "[Epoch 02] Train Loss=0.3200, Acc: 0.8670, F1=0.8186 | Val Loss=0.6285, Acc: 0.7000 F1=0.4902\n",
      "[Epoch 03] Train Loss=0.2847, Acc: 0.8808, F1=0.8393 | Val Loss=0.7496, Acc: 0.7155 F1=0.5290\n",
      "[Epoch 04] Train Loss=0.2733, Acc: 0.8838, F1=0.8448 | Val Loss=0.4535, Acc: 0.7883 F1=0.6862\n",
      "[Epoch 05] Train Loss=0.2596, Acc: 0.8926, F1=0.8574 | Val Loss=0.5740, Acc: 0.7585 F1=0.6142\n",
      "[Epoch 06] Train Loss=0.2468, Acc: 0.8984, F1=0.8659 | Val Loss=0.5303, Acc: 0.7795 F1=0.6711\n",
      "[Epoch 07] Train Loss=0.2360, Acc: 0.9009, F1=0.8696 | Val Loss=0.4374, Acc: 0.8177 F1=0.7452\n",
      "[Epoch 08] Train Loss=0.2292, Acc: 0.9040, F1=0.8738 | Val Loss=0.4670, Acc: 0.8080 F1=0.7187\n",
      "[Epoch 09] Train Loss=0.2239, Acc: 0.9072, F1=0.8779 | Val Loss=0.5001, Acc: 0.7895 F1=0.6830\n",
      "[Epoch 10] Train Loss=0.2176, Acc: 0.9110, F1=0.8834 | Val Loss=0.4067, Acc: 0.8345 F1=0.7698\n",
      "[Epoch 11] Train Loss=0.2095, Acc: 0.9145, F1=0.8881 | Val Loss=0.4221, Acc: 0.8325 F1=0.7694\n",
      "[Epoch 12] Train Loss=0.2059, Acc: 0.9168, F1=0.8911 | Val Loss=0.5063, Acc: 0.8095 F1=0.7292\n",
      "[Epoch 13] Train Loss=0.1981, Acc: 0.9215, F1=0.8974 | Val Loss=0.4970, Acc: 0.8293 F1=0.7655\n",
      "[Epoch 14] Train Loss=0.1972, Acc: 0.9198, F1=0.8951 | Val Loss=0.3634, Acc: 0.8495 F1=0.7984\n",
      "[Epoch 15] Train Loss=0.1931, Acc: 0.9200, F1=0.8955 | Val Loss=0.3323, Acc: 0.8618 F1=0.8229\n",
      "[Epoch 16] Train Loss=0.1848, Acc: 0.9232, F1=0.9000 | Val Loss=0.4282, Acc: 0.8337 F1=0.7722\n",
      "[Epoch 17] Train Loss=0.1803, Acc: 0.9273, F1=0.9053 | Val Loss=0.4585, Acc: 0.8290 F1=0.7630\n",
      "[Epoch 18] Train Loss=0.1798, Acc: 0.9295, F1=0.9081 | Val Loss=0.4056, Acc: 0.8438 F1=0.7951\n",
      "[Epoch 19] Train Loss=0.1765, Acc: 0.9280, F1=0.9062 | Val Loss=0.4678, Acc: 0.8233 F1=0.7585\n",
      "[Epoch 20] Train Loss=0.1741, Acc: 0.9287, F1=0.9070 | Val Loss=0.4050, Acc: 0.8337 F1=0.7709\n",
      "[Epoch 21] Train Loss=0.1684, Acc: 0.9346, F1=0.9146 | Val Loss=0.3565, Acc: 0.8615 F1=0.8230\n",
      "[Epoch 22] Train Loss=0.1547, Acc: 0.9402, F1=0.9219 | Val Loss=0.3348, Acc: 0.8762 F1=0.8427\n",
      "[Epoch 23] Train Loss=0.1522, Acc: 0.9382, F1=0.9195 | Val Loss=0.3815, Acc: 0.8555 F1=0.8106\n",
      "[Epoch 24] Train Loss=0.1507, Acc: 0.9397, F1=0.9216 | Val Loss=0.4232, Acc: 0.8485 F1=0.7936\n",
      "[Epoch 25] Train Loss=0.1456, Acc: 0.9435, F1=0.9265 | Val Loss=0.4222, Acc: 0.8515 F1=0.8031\n",
      "[Epoch 26] Train Loss=0.1448, Acc: 0.9418, F1=0.9241 | Val Loss=0.3595, Acc: 0.8625 F1=0.8207\n",
      "[Epoch 27] Train Loss=0.1401, Acc: 0.9431, F1=0.9260 | Val Loss=0.3690, Acc: 0.8600 F1=0.8215\n",
      "[Epoch 28] Train Loss=0.1359, Acc: 0.9469, F1=0.9310 | Val Loss=0.3832, Acc: 0.8555 F1=0.8105\n",
      "[Epoch 29] Train Loss=0.1348, Acc: 0.9456, F1=0.9294 | Val Loss=0.4154, Acc: 0.8468 F1=0.7955\n",
      "Early stopping at epoch 29 (no val F1 improvement for 7 epochs).\n",
      "============================================================\n",
      "Fold 3/5\n",
      "Train subjects (n=26): ['194d1d', '220a17', '231c3b', '24a59d', '251738', '2a39f8', '2c98f7', '2d57c2', '31d269', '4b39ac', '4ca9b3', '4f13b4', '516a67', '7688c1', '79011a', '7eb666', '7fcee9', '87174c', '8db7dd', 'a03db7', 'bc3908', 'c85fdf', 'c8e721', 'd8836b', 'e8919c', 'f2c8aa']\n",
      "Val subjects   (n=8): ['07285e', '364459', '3b2403', '48fd62', '54ee6e', '66341b', '93f49f', 'd9312a']\n",
      "Train windows: 15909, Val windows: 3971\n",
      "[Epoch 01] Train Loss=0.4499, Acc: 0.7716, F1=0.7290 | Val Loss=0.4115, Acc: 0.8298 F1=0.3172\n",
      "[Epoch 02] Train Loss=0.3305, Acc: 0.8596, F1=0.8401 | Val Loss=0.4263, Acc: 0.8260 F1=0.3324\n",
      "[Epoch 03] Train Loss=0.2886, Acc: 0.8831, F1=0.8682 | Val Loss=0.4170, Acc: 0.8303 F1=0.3405\n",
      "[Epoch 04] Train Loss=0.2661, Acc: 0.8954, F1=0.8826 | Val Loss=0.4461, Acc: 0.8106 F1=0.3139\n",
      "[Epoch 05] Train Loss=0.2488, Acc: 0.9022, F1=0.8904 | Val Loss=0.4318, Acc: 0.8245 F1=0.3431\n",
      "[Epoch 06] Train Loss=0.2379, Acc: 0.9078, F1=0.8969 | Val Loss=0.4416, Acc: 0.8267 F1=0.3359\n",
      "[Epoch 07] Train Loss=0.2282, Acc: 0.9099, F1=0.8992 | Val Loss=0.4517, Acc: 0.7985 F1=0.3856\n",
      "[Epoch 08] Train Loss=0.2118, Acc: 0.9187, F1=0.9094 | Val Loss=0.4561, Acc: 0.8136 F1=0.3369\n",
      "[Epoch 09] Train Loss=0.2068, Acc: 0.9224, F1=0.9134 | Val Loss=0.4374, Acc: 0.8245 F1=0.3266\n",
      "[Epoch 10] Train Loss=0.2025, Acc: 0.9213, F1=0.9122 | Val Loss=0.4675, Acc: 0.8199 F1=0.3324\n",
      "[Epoch 11] Train Loss=0.1968, Acc: 0.9238, F1=0.9150 | Val Loss=0.4393, Acc: 0.8212 F1=0.3522\n",
      "[Epoch 12] Train Loss=0.1919, Acc: 0.9273, F1=0.9191 | Val Loss=0.4553, Acc: 0.8099 F1=0.3682\n",
      "[Epoch 13] Train Loss=0.1885, Acc: 0.9271, F1=0.9188 | Val Loss=0.4468, Acc: 0.8235 F1=0.3539\n",
      "[Epoch 14] Train Loss=0.1810, Acc: 0.9311, F1=0.9233 | Val Loss=0.4501, Acc: 0.8189 F1=0.3470\n",
      "Early stopping at epoch 14 (no val F1 improvement for 7 epochs).\n",
      "============================================================\n",
      "Fold 4/5\n",
      "Train subjects (n=26): ['07285e', '194d1d', '220a17', '231c3b', '251738', '2a39f8', '2c98f7', '2d57c2', '31d269', '364459', '3b2403', '48fd62', '4f13b4', '516a67', '54ee6e', '66341b', '7688c1', '7fcee9', '87174c', '93f49f', 'a03db7', 'bc3908', 'c85fdf', 'd9312a', 'e8919c', 'f2c8aa']\n",
      "Val subjects   (n=8): ['24a59d', '4b39ac', '4ca9b3', '79011a', '7eb666', '8db7dd', 'c8e721', 'd8836b']\n",
      "Train windows: 15916, Val windows: 3964\n",
      "[Epoch 01] Train Loss=0.4534, Acc: 0.7803, F1=0.6889 | Val Loss=0.4670, Acc: 0.8123 F1=0.6995\n",
      "[Epoch 02] Train Loss=0.3494, Acc: 0.8476, F1=0.8043 | Val Loss=0.3765, Acc: 0.8403 F1=0.7501\n",
      "[Epoch 03] Train Loss=0.3103, Acc: 0.8689, F1=0.8354 | Val Loss=0.3495, Acc: 0.8502 F1=0.7583\n",
      "[Epoch 04] Train Loss=0.2882, Acc: 0.8780, F1=0.8476 | Val Loss=0.3603, Acc: 0.8489 F1=0.7313\n",
      "[Epoch 05] Train Loss=0.2698, Acc: 0.8877, F1=0.8606 | Val Loss=0.3180, Acc: 0.8673 F1=0.7879\n",
      "[Epoch 06] Train Loss=0.2609, Acc: 0.8927, F1=0.8672 | Val Loss=0.3153, Acc: 0.8613 F1=0.7705\n",
      "[Epoch 07] Train Loss=0.2493, Acc: 0.8975, F1=0.8732 | Val Loss=0.3106, Acc: 0.8706 F1=0.7952\n",
      "[Epoch 08] Train Loss=0.2365, Acc: 0.9030, F1=0.8802 | Val Loss=0.3357, Acc: 0.8648 F1=0.7837\n",
      "[Epoch 09] Train Loss=0.2333, Acc: 0.9037, F1=0.8811 | Val Loss=0.3091, Acc: 0.8691 F1=0.7860\n",
      "[Epoch 10] Train Loss=0.2287, Acc: 0.9059, F1=0.8842 | Val Loss=0.3243, Acc: 0.8681 F1=0.7939\n",
      "[Epoch 11] Train Loss=0.2237, Acc: 0.9095, F1=0.8885 | Val Loss=0.3236, Acc: 0.8663 F1=0.7954\n",
      "[Epoch 12] Train Loss=0.2166, Acc: 0.9118, F1=0.8915 | Val Loss=0.3180, Acc: 0.8703 F1=0.7946\n",
      "[Epoch 13] Train Loss=0.2112, Acc: 0.9144, F1=0.8950 | Val Loss=0.3285, Acc: 0.8688 F1=0.7969\n",
      "[Epoch 14] Train Loss=0.2059, Acc: 0.9159, F1=0.8967 | Val Loss=0.3317, Acc: 0.8724 F1=0.7921\n",
      "[Epoch 15] Train Loss=0.1995, Acc: 0.9203, F1=0.9024 | Val Loss=0.3406, Acc: 0.8676 F1=0.7778\n",
      "[Epoch 16] Train Loss=0.1884, Acc: 0.9242, F1=0.9071 | Val Loss=0.3580, Acc: 0.8686 F1=0.7810\n",
      "[Epoch 17] Train Loss=0.1839, Acc: 0.9261, F1=0.9096 | Val Loss=0.3648, Acc: 0.8673 F1=0.7924\n",
      "[Epoch 18] Train Loss=0.1777, Acc: 0.9281, F1=0.9121 | Val Loss=0.3465, Acc: 0.8711 F1=0.7893\n",
      "[Epoch 19] Train Loss=0.1759, Acc: 0.9309, F1=0.9153 | Val Loss=0.3666, Acc: 0.8668 F1=0.7901\n",
      "[Epoch 20] Train Loss=0.1768, Acc: 0.9296, F1=0.9139 | Val Loss=0.3552, Acc: 0.8688 F1=0.7910\n",
      "Early stopping at epoch 20 (no val F1 improvement for 7 epochs).\n",
      "============================================================\n",
      "Fold 5/5\n",
      "Train subjects (n=26): ['07285e', '194d1d', '231c3b', '24a59d', '251738', '2a39f8', '2d57c2', '364459', '3b2403', '48fd62', '4b39ac', '4ca9b3', '4f13b4', '54ee6e', '66341b', '7688c1', '79011a', '7eb666', '87174c', '8db7dd', '93f49f', 'c85fdf', 'c8e721', 'd8836b', 'd9312a', 'f2c8aa']\n",
      "Val subjects   (n=8): ['220a17', '2c98f7', '31d269', '516a67', '7fcee9', 'a03db7', 'bc3908', 'e8919c']\n",
      "Train windows: 15919, Val windows: 3961\n",
      "[Epoch 01] Train Loss=0.4870, Acc: 0.7432, F1=0.7060 | Val Loss=0.4234, Acc: 0.8175 F1=0.4696\n",
      "[Epoch 02] Train Loss=0.3485, Acc: 0.8495, F1=0.8261 | Val Loss=0.4270, Acc: 0.8081 F1=0.5208\n",
      "[Epoch 03] Train Loss=0.3040, Acc: 0.8722, F1=0.8539 | Val Loss=0.4089, Acc: 0.8149 F1=0.5200\n",
      "[Epoch 04] Train Loss=0.2808, Acc: 0.8842, F1=0.8686 | Val Loss=0.3988, Acc: 0.8351 F1=0.5462\n",
      "[Epoch 05] Train Loss=0.2615, Acc: 0.8916, F1=0.8775 | Val Loss=0.3868, Acc: 0.8346 F1=0.5705\n",
      "[Epoch 06] Train Loss=0.2485, Acc: 0.8968, F1=0.8834 | Val Loss=0.4691, Acc: 0.7932 F1=0.5571\n",
      "[Epoch 07] Train Loss=0.2362, Acc: 0.9070, F1=0.8952 | Val Loss=0.4122, Acc: 0.8261 F1=0.5755\n",
      "[Epoch 08] Train Loss=0.2288, Acc: 0.9078, F1=0.8963 | Val Loss=0.4627, Acc: 0.8023 F1=0.5648\n",
      "[Epoch 09] Train Loss=0.2223, Acc: 0.9113, F1=0.9004 | Val Loss=0.3751, Acc: 0.8417 F1=0.5845\n",
      "[Epoch 10] Train Loss=0.2184, Acc: 0.9126, F1=0.9018 | Val Loss=0.4462, Acc: 0.8122 F1=0.5385\n",
      "[Epoch 11] Train Loss=0.2135, Acc: 0.9154, F1=0.9052 | Val Loss=0.5397, Acc: 0.7574 F1=0.5469\n",
      "[Epoch 12] Train Loss=0.2037, Acc: 0.9181, F1=0.9084 | Val Loss=0.4413, Acc: 0.8210 F1=0.5842\n",
      "[Epoch 13] Train Loss=0.1991, Acc: 0.9214, F1=0.9120 | Val Loss=0.3870, Acc: 0.8516 F1=0.5579\n",
      "[Epoch 14] Train Loss=0.1951, Acc: 0.9244, F1=0.9152 | Val Loss=0.3929, Acc: 0.8369 F1=0.5848\n",
      "[Epoch 15] Train Loss=0.1912, Acc: 0.9233, F1=0.9142 | Val Loss=0.4965, Acc: 0.7887 F1=0.5652\n",
      "[Epoch 16] Train Loss=0.1785, Acc: 0.9299, F1=0.9215 | Val Loss=0.4075, Acc: 0.8346 F1=0.5799\n",
      "[Epoch 17] Train Loss=0.1789, Acc: 0.9287, F1=0.9203 | Val Loss=0.4558, Acc: 0.8124 F1=0.5500\n",
      "[Epoch 18] Train Loss=0.1707, Acc: 0.9341, F1=0.9264 | Val Loss=0.4422, Acc: 0.8182 F1=0.5477\n",
      "[Epoch 19] Train Loss=0.1686, Acc: 0.9338, F1=0.9260 | Val Loss=0.5144, Acc: 0.8006 F1=0.5720\n",
      "[Epoch 20] Train Loss=0.1647, Acc: 0.9345, F1=0.9269 | Val Loss=0.4739, Acc: 0.8132 F1=0.5471\n",
      "[Epoch 21] Train Loss=0.1634, Acc: 0.9361, F1=0.9285 | Val Loss=0.4964, Acc: 0.8016 F1=0.5624\n",
      "Early stopping at epoch 21 (no val F1 improvement for 7 epochs).\n",
      "üìÅ Saved best model for Fold 0 ‚Üí all_best_models\\best_model_fold_0.pt\n",
      "üìÅ Saved best model for Fold 1 ‚Üí all_best_models\\best_model_fold_1.pt\n",
      "üìÅ Saved best model for Fold 2 ‚Üí all_best_models\\best_model_fold_2.pt\n",
      "üìÅ Saved best model for Fold 3 ‚Üí all_best_models\\best_model_fold_3.pt\n",
      "üìÅ Saved best model for Fold 4 ‚Üí all_best_models\\best_model_fold_4.pt\n",
      "‚úÖ Fold metrics saved to Excel: fog_cv_folds.xlsx\n",
      "‚úÖ Summary metrics saved to Excel: fog_cv_summary.xlsx\n",
      "üèÜ Global best model (Fold 0) saved ‚Üí best_fog_model.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cv_results = cross_validate_patient_independent(\n",
    "    train_df,\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    input_dim=3,\n",
    "    hidden_dim=64,\n",
    "    num_layers=1,\n",
    "    bidirectional=False,\n",
    "    dropout=0.2,\n",
    "    batch_size=16,\n",
    "    num_epochs=50,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    early_stopping_patience=7,\n",
    "    device=None,  # auto: cuda if available else cpu\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e820ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, average_precision_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_models_on_test_ensemble(\n",
    "    test_df: pd.DataFrame,\n",
    "    model_paths: List[str],\n",
    "    batch_size: int = 32,\n",
    "    device: str = None,\n",
    "    fold_weights: List[float] = None   # OPTIONAL for weighted voting\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate an ensemble of models on the test set using:\n",
    "        - Soft voting (default)\n",
    "        - Hard majority voting\n",
    "        - Optional weighted voting\n",
    "\n",
    "    Args:\n",
    "        test_df        : Test dataframe\n",
    "        model_paths    : List of paths to saved fold models\n",
    "        batch_size     : Test batch size\n",
    "        device         : 'cpu' or 'cuda'\n",
    "        fold_weights   : Optional weights per fold (e.g. fold F1)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with all ensemble metrics and voting predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Device Setup\n",
    "    # -----------------------------\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # -----------------------------\n",
    "    # DataLoader\n",
    "    # -----------------------------\n",
    "    test_loader = make_dataloader(test_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # collect predictions from each model\n",
    "    prob_list = []     # soft voting\n",
    "    hard_list = []     # hard voting\n",
    "    targets_list = []\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load each model and predict\n",
    "    # -----------------------------\n",
    "    for idx, model_path in enumerate(model_paths):\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"‚ùå Model not found: {model_path}\")\n",
    "\n",
    "        print(f\" Loading model: {model_path}\")\n",
    "\n",
    "        model = TCN().to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        fold_probs = []\n",
    "        fold_hard = []\n",
    "        fold_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                logits = model(X)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "                fold_probs.extend(probs)\n",
    "                fold_hard.extend(preds)\n",
    "                fold_targets.extend(y.cpu().numpy().astype(int))\n",
    "\n",
    "        prob_list.append(np.array(fold_probs))\n",
    "        hard_list.append(np.array(fold_hard))\n",
    "        targets_list = fold_targets     # same for all folds\n",
    "\n",
    "    prob_matrix = np.vstack(prob_list)     # shape: (num_models, N)\n",
    "    hard_matrix = np.vstack(hard_list)     # shape: (num_models, N)\n",
    "    ground_truth = np.array(targets_list)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Voting Methods\n",
    "    # -----------------------------\n",
    "\n",
    "    # SOFT VOTING (best default)\n",
    "    soft_probs = prob_matrix.mean(axis=0)\n",
    "    soft_preds = (soft_probs >= 0.5).astype(int)\n",
    "\n",
    "    # HARD VOTING\n",
    "    hard_preds = np.round(hard_matrix.mean(axis=0)).astype(int)\n",
    "\n",
    "    # WEIGHTED VOTING (if provided)\n",
    "    if fold_weights is not None:\n",
    "        w = np.array(fold_weights).reshape(-1, 1)\n",
    "        weighted_probs = (prob_matrix * w).sum(axis=0) / w.sum()\n",
    "        weighted_preds = (weighted_probs >= 0.5).astype(int)\n",
    "    else:\n",
    "        weighted_preds = None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Metric Function\n",
    "    # -----------------------------\n",
    "    def compute_metrics(preds, probs=None):\n",
    "        return {\n",
    "            \"loss\": criterion(\n",
    "                torch.tensor(preds, dtype=torch.float32),\n",
    "                torch.tensor(ground_truth, dtype=torch.float32)\n",
    "            ).item(),\n",
    "            \"accuracy\": accuracy_score(ground_truth, preds),\n",
    "            \"precision\": precision_score(ground_truth, preds, zero_division=0),\n",
    "            \"recall\": recall_score(ground_truth, preds, zero_division=0),\n",
    "            \"f1\": f1_score(ground_truth, preds, zero_division=0),\n",
    "            \"roc_auc\": roc_auc_score(ground_truth, probs) if probs is not None else None,\n",
    "            \"pr_auc\": average_precision_score(ground_truth, probs) if probs is not None else None,\n",
    "            \"confusion_matrix\": confusion_matrix(ground_truth, preds)\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Compute Metrics\n",
    "    # -----------------------------\n",
    "    metrics_soft = compute_metrics(soft_preds, soft_probs)\n",
    "    metrics_hard = compute_metrics(hard_preds, soft_probs)\n",
    "    metrics_weighted = compute_metrics(weighted_preds, weighted_probs) if weighted_preds is not None else None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Print Results\n",
    "    # -----------------------------\n",
    "    print(\"\\n\\n **SOFT VOTING RESULTS**\")\n",
    "    for k, v in metrics_soft.items():\n",
    "        if k != \"confusion_matrix\":\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    print(\"\\n **HARD VOTING RESULTS**\")\n",
    "    for k, v in metrics_hard.items():\n",
    "        if k != \"confusion_matrix\":\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    if metrics_weighted is not None:\n",
    "        print(\"\\n **WEIGHTED VOTING RESULTS**\")\n",
    "        for k, v in metrics_weighted.items():\n",
    "            if k != \"confusion_matrix\":\n",
    "                print(f\"{k}: {v}\")\n",
    "\n",
    "    return {\n",
    "        \"metrics_soft\": metrics_soft,\n",
    "        \"metrics_hard\": metrics_hard,\n",
    "        \"metrics_weighted\": metrics_weighted,\n",
    "        \"soft_preds\": soft_preds,\n",
    "        \"soft_probs\": soft_probs,\n",
    "        \"hard_preds\": hard_preds,\n",
    "        \"weighted_preds\": weighted_preds\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "828f2b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\all_best_models\\\\best_model_fold_0.pt',\n",
       " '.\\\\all_best_models\\\\best_model_fold_1.pt',\n",
       " '.\\\\all_best_models\\\\best_model_fold_2.pt',\n",
       " '.\\\\all_best_models\\\\best_model_fold_3.pt',\n",
       " '.\\\\all_best_models\\\\best_model_fold_4.pt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = '.\\\\all_best_models\\\\'\n",
    "file_paths = []\n",
    "for root, _, files in os.walk(directory_path):\n",
    "    for file in files:\n",
    "        file_paths.append(os.path.join(root, file))\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e30d0a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading model: .\\all_best_models\\best_model_fold_0.pt\n",
      "üì• Loading model: .\\all_best_models\\best_model_fold_1.pt\n",
      "üì• Loading model: .\\all_best_models\\best_model_fold_2.pt\n",
      "üì• Loading model: .\\all_best_models\\best_model_fold_3.pt\n",
      "üì• Loading model: .\\all_best_models\\best_model_fold_4.pt\n",
      "\n",
      "\n",
      "üéØ **SOFT VOTING RESULTS**\n",
      "loss: 0.7006181478500366\n",
      "accuracy: 0.8816986855409504\n",
      "precision: 0.5749235474006116\n",
      "recall: 0.6643109540636042\n",
      "f1: 0.6163934426229508\n",
      "roc_auc: 0.9036972179659567\n",
      "pr_auc: 0.6407261323034097\n",
      "\n",
      "üó≥Ô∏è **HARD VOTING RESULTS**\n",
      "loss: 0.7043604850769043\n",
      "accuracy: 0.8766430738119313\n",
      "precision: 0.5561959654178674\n",
      "recall: 0.6819787985865724\n",
      "f1: 0.6126984126984127\n",
      "roc_auc: 0.9036972179659567\n",
      "pr_auc: 0.6407261323034097\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_models_on_test_ensemble(\n",
    "    test_df=test_df,\n",
    "    model_paths= file_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71094667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  **HARD VOTING RESULTS**\n",
    "# loss: 0.7022083401679993\n",
    "# accuracy: 0.8842264914054601\n",
    "# precision: 0.5721925133689839\n",
    "# recall: 0.7561837455830389\n",
    "# f1: 0.6514459665144596\n",
    "# roc_auc: 0.9207417367647519\n",
    "# pr_auc: 0.708372352847281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c584483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve\n",
    "\n",
    "\n",
    "def save_and_plot_ensemble_results(\n",
    "    eval_results: Dict[str, Any],\n",
    "    ground_truth: np.ndarray,\n",
    "    output_folder: str = \"ensemble_results/\",\n",
    "    model_output_path: str = \"ensemble_final_model.pt\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates plots (confusion matrix, ROC, PR curve),\n",
    "    saves predictions, and exports the ensemble model.\n",
    "\n",
    "    Args:\n",
    "        eval_results      : Output of evaluate_models_on_test_ensemble()\n",
    "        ground_truth      : Numpy array of true labels\n",
    "        output_folder     : Directory to save images & CSV\n",
    "        model_output_path : File to save final ensemble soft-voting model weights\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract predictions\n",
    "    # -----------------------------\n",
    "    soft_probs = eval_results[\"soft_probs\"]\n",
    "    soft_preds = eval_results[\"soft_preds\"]\n",
    "    hard_preds = eval_results[\"hard_preds\"]\n",
    "    weighted_preds = eval_results[\"weighted_preds\"]\n",
    "\n",
    "    # =============================\n",
    "    #  1. Save predictions to CSV\n",
    "    # =============================\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"y_true\": ground_truth,\n",
    "        \"soft_prob\": soft_probs,\n",
    "        \"soft_pred\": soft_preds,\n",
    "        \"hard_pred\": hard_preds,\n",
    "        \"weighted_pred\": weighted_preds if weighted_preds is not None else np.nan\n",
    "    })\n",
    "\n",
    "    csv_path = os.path.join(output_folder, \"ensemble_predictions.csv\")\n",
    "    pred_df.to_csv(csv_path, index=False)\n",
    "    print(f\" Predictions saved to: {csv_path}\")\n",
    "\n",
    "    # =============================\n",
    "    #  2. Confusion Matrix Plot\n",
    "    # =============================\n",
    "    cm = confusion_matrix(ground_truth, soft_preds)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.imshow(cm, cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks([0, 1])\n",
    "    plt.yticks([0, 1])\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, str(cm[i, j]), ha='center', va='center', color='red')\n",
    "\n",
    "    cm_path = os.path.join(output_folder, \"confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\" Confusion Matrix saved to: {cm_path}\")\n",
    "\n",
    "    # =============================\n",
    "    #  3. ROC Curve Plot\n",
    "    # =============================\n",
    "    fpr, tpr, _ = roc_curve(ground_truth, soft_probs)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, label=\"ROC\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Soft Voting)\")\n",
    "    plt.legend()\n",
    "\n",
    "    roc_path = os.path.join(output_folder, \"roc_curve.png\")\n",
    "    plt.savefig(roc_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"ROC Curve saved to: {roc_path}\")\n",
    "\n",
    "    # =============================\n",
    "    #  4. Precision‚ÄìRecall Curve\n",
    "    # =============================\n",
    "    precision, recall, _ = precision_recall_curve(ground_truth, soft_probs)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(recall, precision, label=\"Precision‚ÄìRecall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision‚ÄìRecall Curve (Soft Voting)\")\n",
    "    plt.legend()\n",
    "\n",
    "    pr_path = os.path.join(output_folder, \"pr_curve.png\")\n",
    "    plt.savefig(pr_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Precision‚ÄìRecall Curve saved to: {pr_path}\")\n",
    "\n",
    "    # ============================\n",
    "    #  5. Export Final Ensemble Model\n",
    "    # =============================\n",
    "\n",
    "    \"\"\"\n",
    "    Ensemble model: soft-voting means averaging probabilities.\n",
    "    You cannot save a single PyTorch state dict unless we create\n",
    "    a small wrapper module below.\n",
    "    \"\"\"\n",
    "\n",
    "    class SoftVotingEnsemble(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, prob_list):\n",
    "            \"\"\"\n",
    "            prob_list: tensor shape (num_models, batch_size)\n",
    "            \"\"\"\n",
    "            return prob_list.mean(dim=0)\n",
    "\n",
    "    ensemble_model = SoftVotingEnsemble()\n",
    "    torch.save(ensemble_model.state_dict(), model_output_path)\n",
    "\n",
    "    print(f\" Final Ensemble Model saved to: {model_output_path}\")\n",
    "\n",
    "    print(\"\\n ALL RESULTS SAVED SUCCESSFULLY!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1af3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['window_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b793e153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Predictions saved to: ensemble_results/ensemble_predictions.csv\n",
      "üìä Confusion Matrix saved to: ensemble_results/confusion_matrix.png\n",
      "üìà ROC Curve saved to: ensemble_results/roc_curve.png\n",
      "üìâ Precision‚ÄìRecall Curve saved to: ensemble_results/pr_curve.png\n",
      "üß† Final Ensemble Model saved to: ensemble_final_soft_voting.pt\n",
      "\n",
      "üéâ ALL RESULTS SAVED SUCCESSFULLY!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ground_truth = test_df['window_label'].values  # or however your labels stored\n",
    "\n",
    "save_and_plot_ensemble_results(\n",
    "    eval_results=results,\n",
    "    ground_truth=ground_truth,\n",
    "    output_folder=\"ensemble_results/\",\n",
    "    model_output_path=\"ensemble_final_soft_voting.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b285025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
