{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f488a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset & DataLoader helpers\n",
    "# -----------------------------\n",
    "\n",
    "class FoGWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for window-level FoG data.\n",
    "\n",
    "    Expects a DataFrame with columns:\n",
    "        - 'sequence'     : (T, 3) accel window (list/array/string)\n",
    "        - 'window_label' : 0/1\n",
    "        - 'subject'      : patient ID (not used in __getitem__, but kept in df)\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def _parse_sequence(self, seq_obj):\n",
    "        # Handles numpy array, python list, or string from CSV\n",
    "        if isinstance(seq_obj, np.ndarray):\n",
    "            arr = seq_obj.astype(np.float32)\n",
    "        elif isinstance(seq_obj, list):\n",
    "            arr = np.asarray(seq_obj, dtype=np.float32)\n",
    "        else:\n",
    "            # assume string like \"[[...], [...], ...]\"\n",
    "            arr = np.array(ast.literal_eval(seq_obj), dtype=np.float32)\n",
    "\n",
    "        # Ensure shape (T, 3)\n",
    "        if arr.ndim == 1:\n",
    "            arr = arr.reshape(-1, 1)\n",
    "        return arr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq = self._parse_sequence(row['sequence'])  # (T, 3)\n",
    "        label = float(row['window_label'])\n",
    "\n",
    "        x = torch.from_numpy(seq)               # (T, 3)\n",
    "        y = torch.tensor(label, dtype=torch.float32)  # scalar 0/1\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def make_dataloader(df: pd.DataFrame,\n",
    "                    batch_size: int = 64,\n",
    "                    shuffle: bool = True,\n",
    "                    num_workers: int = 0) -> DataLoader:\n",
    "    dataset = FoGWindowDataset(df)\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        num_workers=num_workers)\n",
    "    return loader\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Patient-independent folds\n",
    "# -----------------------------\n",
    "\n",
    "def create_group_kfold_splits(df: pd.DataFrame,\n",
    "                              n_splits: int = 5,\n",
    "                              random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Patient-independent K-fold splits using GroupKFold on 'subject'.\n",
    "\n",
    "    Returns a list of (train_idx, val_idx) index arrays.\n",
    "    \"\"\"\n",
    "    groups = df['subject'].values\n",
    "    y = df['window_label'].values\n",
    "    X = np.arange(len(df))\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    # GroupKFold is deterministic; we can shuffle subjects before if needed:\n",
    "    # but simplest is to just use GroupKFold directly.\n",
    "    splits = list(gkf.split(X, y, groups))\n",
    "    return splits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics helper\n",
    "# -----------------------------\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray,\n",
    "                    y_pred_probs: np.ndarray,\n",
    "                    threshold: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, recall, F1 for binary classification.\n",
    "    \"\"\"\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='binary', zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'accuracy': float(acc),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1': float(f1),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# One epoch train / eval\n",
    "# -----------------------------\n",
    "\n",
    "def run_epoch(model: nn.Module,\n",
    "              loader: DataLoader,\n",
    "              criterion,\n",
    "              device: torch.device,\n",
    "              optimizer=None) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    If optimizer is provided: training mode.\n",
    "    Otherwise: evaluation mode.\n",
    "\n",
    "    Returns dict: loss, accuracy, precision, recall, f1\n",
    "    \"\"\"\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "    else:\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "    all_losses = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)              # (batch, T, 3)\n",
    "        y = y.to(device)              # (batch,)\n",
    "\n",
    "        logits = model(x)             # (batch,)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        all_probs.append(probs.detach().cpu().numpy())\n",
    "        all_labels.append(y.detach().cpu().numpy())\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    metrics = compute_metrics(all_labels, all_probs)\n",
    "    metrics['loss'] = float(np.mean(all_losses))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training one fold (with early stopping & scheduler)\n",
    "# -----------------------------\n",
    "\n",
    "def train_one_fold(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    input_dim: int = 3,\n",
    "    hidden_dim: int = 128,\n",
    "    num_layers: int = 2,\n",
    "    bidirectional: bool = False,\n",
    "    dropout: float = 0.0,\n",
    "    batch_size: int = 16,\n",
    "    num_epochs: int = 50,\n",
    "    lr: float = 1e-4,\n",
    "    weight_decay: float = 1e-4,\n",
    "    early_stopping_patience: int = 7,\n",
    "    device: str = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train LSTM on one fold with early stopping on val F1 and\n",
    "    ReduceLROnPlateau scheduler on val loss.\n",
    "\n",
    "    Returns:\n",
    "        dict with:\n",
    "            - 'best_state_dict'\n",
    "            - 'history' : list of per-epoch metrics\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = make_dataloader(train_df, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = make_dataloader(val_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model, loss, optimizer, scheduler\n",
    "    #model =  ParallelCNNLSTMTransformer().to(device)\n",
    "    # ---------------------------\n",
    "    # Quick test\n",
    "    # ---------------------------\n",
    "\n",
    "    B = 4\n",
    "    T = 256\n",
    "    C = 3\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = LSTMTransformer().to(device)\n",
    "\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    #criterion = WeightedBCEWithLogitsLoss()\n",
    "    #criterion = FocalTverskyLoss(gamma=0.75, alpha=0.7)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    best_val_f1 = -np.inf\n",
    "    best_state_dict = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    history = []  # list of dicts per epoch\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # --- Train ---\n",
    "        train_metrics = run_epoch(model, train_loader, criterion, device, optimizer)\n",
    "\n",
    "        # --- Validation ---\n",
    "        val_metrics = run_epoch(model, val_loader, criterion, device, optimizer=None)\n",
    "\n",
    "        # Step scheduler on validation loss\n",
    "        scheduler.step(val_metrics['loss'])\n",
    "\n",
    "        # Early stopping on val F1\n",
    "        val_f1 = val_metrics['f1']\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Record metrics\n",
    "        epoch_record = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_metrics['loss'],\n",
    "            'train_accuracy': train_metrics['accuracy'],\n",
    "            'train_precision': train_metrics['precision'],\n",
    "            'train_recall': train_metrics['recall'],\n",
    "            'train_f1': train_metrics['f1'],\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_accuracy': val_metrics['accuracy'],\n",
    "            'val_precision': val_metrics['precision'],\n",
    "            'val_recall': val_metrics['recall'],\n",
    "            'val_f1': val_metrics['f1'],\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "        }\n",
    "        history.append(epoch_record)\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch:02d}] \"\n",
    "            f\"Train Loss={train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}, F1={train_metrics['f1']:.4f} | \"\n",
    "\n",
    "            f\"Val Loss={val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f} F1={val_metrics['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no val F1 improvement for \"\n",
    "                  f\"{early_stopping_patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'best_state_dict': best_state_dict,\n",
    "        'history': history,\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize_and_save_cv_results(\n",
    "        results: Dict[str, Any],\n",
    "        output_prefix: str = \"cv_results\",\n",
    "        save_best_model_path: str = \"best_model_overall.pt\",\n",
    "        save_all_folds_dir: str = \"all_best_models\"):\n",
    "    \"\"\"\n",
    "    - Extract best metrics from each fold\n",
    "    - Compute mean and std\n",
    "    - Save fold-wise results and summary to Excel\n",
    "    - Save ALL best model weights for all folds\n",
    "    - Save the global best model across folds\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    os.makedirs(save_all_folds_dir, exist_ok=True)\n",
    "\n",
    "    fold_best_rows = []\n",
    "    global_best_f1 = -np.inf\n",
    "    global_best_state_dict = None\n",
    "    global_best_fold = None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract best epoch per fold\n",
    "    # -----------------------------\n",
    "    for fold_data in results['folds']:\n",
    "        fold_idx = fold_data['fold_idx']\n",
    "        history = fold_data['history']\n",
    "\n",
    "        # Best epoch based on val_f1\n",
    "        best_epoch_record = max(history, key=lambda x: x['val_f1'])\n",
    "\n",
    "        row = {\n",
    "            'fold': fold_idx,\n",
    "            'val_accuracy': best_epoch_record['val_accuracy'],\n",
    "            'val_precision': best_epoch_record['val_precision'],\n",
    "            'val_recall': best_epoch_record['val_recall'],\n",
    "            'val_f1': best_epoch_record['val_f1'],\n",
    "            'val_loss': best_epoch_record['val_loss'],\n",
    "        }\n",
    "\n",
    "        fold_best_rows.append(row)\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # Save THIS FOLD'S best model separately\n",
    "        # ---------------------------------------\n",
    "        fold_model_path = os.path.join(\n",
    "            save_all_folds_dir, f\"best_model_fold_{fold_idx}.pt\"\n",
    "        )\n",
    "        torch.save(fold_data['best_state_dict'], fold_model_path)\n",
    "        print(f\"üìÅ Saved best model for Fold {fold_idx} ‚Üí {fold_model_path}\")\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # Track the GLOBAL best model\n",
    "        # ---------------------------------------\n",
    "        if best_epoch_record['val_f1'] > global_best_f1:\n",
    "            global_best_f1 = best_epoch_record['val_f1']\n",
    "            global_best_state_dict = fold_data['best_state_dict']\n",
    "            global_best_fold = fold_idx\n",
    "\n",
    "    df_folds = pd.DataFrame(fold_best_rows)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Mean & Std\n",
    "    # -----------------------------\n",
    "    summary = {}\n",
    "    for col in ['val_accuracy', 'val_precision', 'val_recall', 'val_f1', 'val_loss']:\n",
    "        summary[f'{col}_mean'] = df_folds[col].mean()\n",
    "        summary[f'{col}_std'] = df_folds[col].std()\n",
    "\n",
    "    df_summary = pd.DataFrame([summary])\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save Excel files\n",
    "    # -----------------------------\n",
    "    folds_excel = f\"{output_prefix}_folds.xlsx\"\n",
    "    summary_excel = f\"{output_prefix}_summary.xlsx\"\n",
    "\n",
    "    df_folds.to_excel(folds_excel, index=False)\n",
    "    df_summary.to_excel(summary_excel, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Fold metrics saved to Excel: {folds_excel}\")\n",
    "    print(f\"‚úÖ Summary metrics saved to Excel: {summary_excel}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save global best model\n",
    "    # -----------------------------\n",
    "    if global_best_state_dict is not None:\n",
    "        torch.save(global_best_state_dict, save_best_model_path)\n",
    "        print(f\"üèÜ Global best model (Fold {global_best_fold}) saved ‚Üí {save_best_model_path}\")\n",
    "\n",
    "    return {\n",
    "        'fold_metrics_df': df_folds,\n",
    "        'summary_df': df_summary,\n",
    "        'best_fold': global_best_fold,\n",
    "        'best_f1': global_best_f1\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Full K-fold cross-validation\n",
    "# -----------------------------\n",
    "\n",
    "def cross_validate_patient_independent(\n",
    "    df: pd.DataFrame,\n",
    "    n_splits: int = 5,\n",
    "    random_state: int = 42,\n",
    "    **train_kwargs\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run patient-independent K-fold cross-validation.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          'folds': [\n",
    "            {\n",
    "              'fold_idx': 0,\n",
    "              'train_subjects': [...],\n",
    "              'val_subjects': [...],\n",
    "              'history': [...],           # list of per-epoch dicts\n",
    "              'best_state_dict': {...},\n",
    "            },\n",
    "            ...\n",
    "          ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    splits = create_group_kfold_splits(df, n_splits=n_splits, random_state=random_state)\n",
    "\n",
    "    results = {'folds': []}\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(splits):\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Fold {fold_idx + 1}/{n_splits}\")\n",
    "\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_subjects = sorted(train_df['subject'].unique())\n",
    "        val_subjects = sorted(val_df['subject'].unique())\n",
    "\n",
    "        print(f\"Train subjects (n={len(train_subjects)}): {train_subjects}\")\n",
    "        print(f\"Val subjects   (n={len(val_subjects)}): {val_subjects}\")\n",
    "        print(f\"Train windows: {len(train_df)}, Val windows: {len(val_df)}\")\n",
    "\n",
    "        fold_result = train_one_fold(\n",
    "            train_df=train_df,\n",
    "            val_df=val_df,\n",
    "            **train_kwargs\n",
    "        )\n",
    "\n",
    "        results['folds'].append({\n",
    "            'fold_idx': fold_idx,\n",
    "            'train_subjects': train_subjects,\n",
    "            'val_subjects': val_subjects,\n",
    "            'history': fold_result['history'],\n",
    "            'best_state_dict': fold_result['best_state_dict'],\n",
    "        })\n",
    "    #save the results:\n",
    "    summary = summarize_and_save_cv_results(\n",
    "        results,\n",
    "        output_prefix=\"fog_cv\",\n",
    "        save_best_model_path=\"best_fog_model.pt\"\n",
    "    )\n",
    "\n",
    "    results['summary'] = summary\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ecf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "# --- 4. AttentionLayer (No Change) ---\n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Attention mechanism for global context pooling.\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, hidden_dim)\n",
    "        scores = torch.relu(self.attention(x)) \n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1) \n",
    "        context = torch.sum(x * weights.unsqueeze(-1), dim=1) \n",
    "        return context\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# LSTM branch (stacked LSTM)\n",
    "# ---------------------------\n",
    "class LSTMBranch(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_size=128, num_layers=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=in_channels, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:  (B, T, C)\n",
    "        out, _ = self.lstm(x)  # (B, T, hidden)\n",
    "        return out  # (B, T, hidden)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 lstm_hidden=96,\n",
    "                 lstm_layers=2,\n",
    "           ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.lstm = LSTMBranch(in_channels=in_channels, hidden_size=lstm_hidden,\n",
    "                               num_layers=lstm_layers)\n",
    "\n",
    "\n",
    "\n",
    "        # ===  Attention pooling instead of GlobalAveragePool ===\n",
    "        self.attention = AttentionLayer(hidden_dim=lstm_hidden)\n",
    "\n",
    "        # === Final classifier ===\n",
    "        self.pool_dropout = nn.Dropout(0.2)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "\n",
    "\n",
    "        l = self.lstm(x)           # (B, T, lstm_hidden)\n",
    "\n",
    "\n",
    "        # ===  Attention pooling (replace mean pooling) ===\n",
    "        context = self.attention(l)           # (B, 2E)\n",
    "        context = self.pool_dropout(context)\n",
    "\n",
    "        out = self.mlp(context).squeeze(-1)   # (B,)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c179b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Student\\\\Desktop\\\\Abouhashem\\\\DeepLearningProject\\\\'\n",
    "train_df = pd.read_pickle(path+ \"FoG_windows_train.pkl\")\n",
    "test_df  = pd.read_pickle(path+\"FoG_windows_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17cc619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Fold 1/5\n",
      "Train subjects (n=32): ['07285e', '194d1d', '220a17', '231c3b', '24a59d', '251738', '2a39f8', '2c98f7', '31d269', '364459', '3b2403', '48fd62', '4b39ac', '4ca9b3', '4f13b4', '516a67', '54ee6e', '66341b', '7688c1', '79011a', '7eb666', '7fcee9', '8db7dd', '93f49f', 'a03db7', 'bc3908', 'c85fdf', 'c8e721', 'd8836b', 'd9312a', 'e8919c', 'f2c8aa']\n",
      "Val subjects   (n=2): ['2d57c2', '87174c']\n",
      "Train windows: 15896, Val windows: 3984\n",
      "[Epoch 01] Train Loss=0.5656, Acc: 0.7312, F1=0.0000 | Val Loss=1.4262, Acc: 0.1044 F1=0.0000\n",
      "[Epoch 02] Train Loss=0.5226, Acc: 0.7312, F1=0.0000 | Val Loss=1.0802, Acc: 0.1047 F1=0.0006\n",
      "[Epoch 03] Train Loss=0.5003, Acc: 0.7310, F1=0.1351 | Val Loss=0.9870, Acc: 0.3110 F1=0.3979\n",
      "[Epoch 04] Train Loss=0.4792, Acc: 0.7391, F1=0.4745 | Val Loss=1.0259, Acc: 0.2889 F1=0.3641\n",
      "[Epoch 05] Train Loss=0.4657, Acc: 0.7431, F1=0.5059 | Val Loss=0.8074, Acc: 0.4468 F1=0.5697\n",
      "[Epoch 06] Train Loss=0.4586, Acc: 0.7428, F1=0.4974 | Val Loss=0.7355, Acc: 0.6305 F1=0.7497\n",
      "[Epoch 07] Train Loss=0.4505, Acc: 0.7523, F1=0.5474 | Val Loss=0.9393, Acc: 0.4965 F1=0.6224\n",
      "[Epoch 08] Train Loss=0.4436, Acc: 0.7546, F1=0.5493 | Val Loss=0.6625, Acc: 0.7239 F1=0.8246\n",
      "[Epoch 09] Train Loss=0.4357, Acc: 0.7677, F1=0.5200 | Val Loss=0.6188, Acc: 0.7118 F1=0.8101\n",
      "[Epoch 10] Train Loss=0.4240, Acc: 0.7862, F1=0.4978 | Val Loss=0.5055, Acc: 0.8020 F1=0.8773\n",
      "[Epoch 11] Train Loss=0.4158, Acc: 0.7994, F1=0.5589 | Val Loss=0.8114, Acc: 0.5956 F1=0.7099\n",
      "[Epoch 12] Train Loss=0.4137, Acc: 0.8050, F1=0.6034 | Val Loss=0.6996, Acc: 0.6662 F1=0.7785\n",
      "[Epoch 13] Train Loss=0.4053, Acc: 0.8081, F1=0.6032 | Val Loss=0.5412, Acc: 0.7854 F1=0.8660\n",
      "[Epoch 14] Train Loss=0.3952, Acc: 0.8159, F1=0.6057 | Val Loss=0.4849, Acc: 0.7934 F1=0.8721\n",
      "[Epoch 15] Train Loss=0.4073, Acc: 0.8121, F1=0.6009 | Val Loss=0.5524, Acc: 0.7452 F1=0.8368\n",
      "[Epoch 16] Train Loss=0.3879, Acc: 0.8226, F1=0.6360 | Val Loss=0.5334, Acc: 0.7615 F1=0.8498\n",
      "[Epoch 17] Train Loss=0.3824, Acc: 0.8295, F1=0.6551 | Val Loss=0.4241, Acc: 0.8311 F1=0.8980\n",
      "[Epoch 18] Train Loss=0.3879, Acc: 0.8214, F1=0.6455 | Val Loss=0.5910, Acc: 0.7324 F1=0.8283\n",
      "[Epoch 19] Train Loss=0.4170, Acc: 0.7776, F1=0.6191 | Val Loss=0.6483, Acc: 0.7123 F1=0.8134\n",
      "[Epoch 20] Train Loss=0.4061, Acc: 0.7866, F1=0.5960 | Val Loss=0.4463, Acc: 0.8205 F1=0.8907\n",
      "[Epoch 21] Train Loss=0.3891, Acc: 0.8171, F1=0.6298 | Val Loss=0.4430, Acc: 0.7899 F1=0.8694\n",
      "[Epoch 22] Train Loss=0.3784, Acc: 0.8294, F1=0.6492 | Val Loss=0.4654, Acc: 0.7771 F1=0.8602\n",
      "[Epoch 23] Train Loss=0.3697, Acc: 0.8334, F1=0.6546 | Val Loss=0.4635, Acc: 0.7794 F1=0.8611\n",
      "[Epoch 24] Train Loss=0.3489, Acc: 0.8460, F1=0.6905 | Val Loss=0.3989, Acc: 0.8135 F1=0.8864\n",
      "Early stopping at epoch 24 (no val F1 improvement for 7 epochs).\n",
      "============================================================\n",
      "Fold 2/5\n",
      "Train subjects (n=26): ['07285e', '220a17', '24a59d', '2c98f7', '2d57c2', '31d269', '364459', '3b2403', '48fd62', '4b39ac', '4ca9b3', '516a67', '54ee6e', '66341b', '79011a', '7eb666', '7fcee9', '87174c', '8db7dd', '93f49f', 'a03db7', 'bc3908', 'c8e721', 'd8836b', 'd9312a', 'e8919c']\n",
      "Val subjects   (n=8): ['194d1d', '231c3b', '251738', '2a39f8', '4f13b4', '7688c1', 'c85fdf', 'f2c8aa']\n",
      "Train windows: 15880, Val windows: 4000\n",
      "[Epoch 01] Train Loss=0.6106, Acc: 0.6749, F1=0.3859 | Val Loss=0.6946, Acc: 0.5510 F1=0.0685\n",
      "[Epoch 02] Train Loss=0.4607, Acc: 0.7964, F1=0.7064 | Val Loss=0.6291, Acc: 0.6030 F1=0.3161\n",
      "[Epoch 03] Train Loss=0.4061, Acc: 0.8247, F1=0.7494 | Val Loss=0.5985, Acc: 0.6252 F1=0.3405\n",
      "[Epoch 04] Train Loss=0.3850, Acc: 0.8361, F1=0.7679 | Val Loss=0.6138, Acc: 0.6275 F1=0.3539\n",
      "[Epoch 05] Train Loss=0.3726, Acc: 0.8409, F1=0.7764 | Val Loss=0.5960, Acc: 0.6460 F1=0.3827\n",
      "[Epoch 06] Train Loss=0.3722, Acc: 0.8451, F1=0.7826 | Val Loss=0.6642, Acc: 0.6052 F1=0.2449\n",
      "[Epoch 07] Train Loss=0.3529, Acc: 0.8480, F1=0.7892 | Val Loss=0.4845, Acc: 0.7430 F1=0.6305\n",
      "[Epoch 08] Train Loss=0.3436, Acc: 0.8523, F1=0.7972 | Val Loss=0.6189, Acc: 0.6625 F1=0.4032\n",
      "[Epoch 09] Train Loss=0.3370, Acc: 0.8579, F1=0.8061 | Val Loss=0.6086, Acc: 0.6667 F1=0.4161\n",
      "[Epoch 10] Train Loss=0.3243, Acc: 0.8625, F1=0.8143 | Val Loss=0.5984, Acc: 0.6947 F1=0.4771\n",
      "[Epoch 11] Train Loss=0.3163, Acc: 0.8656, F1=0.8196 | Val Loss=0.5686, Acc: 0.7235 F1=0.5876\n",
      "[Epoch 12] Train Loss=0.3160, Acc: 0.8635, F1=0.8169 | Val Loss=0.6082, Acc: 0.6620 F1=0.4177\n",
      "[Epoch 13] Train Loss=0.3185, Acc: 0.8607, F1=0.8117 | Val Loss=0.7374, Acc: 0.6168 F1=0.2316\n",
      "[Epoch 14] Train Loss=0.3196, Acc: 0.8608, F1=0.8086 | Val Loss=0.5851, Acc: 0.6855 F1=0.4741\n",
      "Early stopping at epoch 14 (no val F1 improvement for 7 epochs).\n",
      "============================================================\n",
      "Fold 3/5\n",
      "Train subjects (n=26): ['194d1d', '220a17', '231c3b', '24a59d', '251738', '2a39f8', '2c98f7', '2d57c2', '31d269', '4b39ac', '4ca9b3', '4f13b4', '516a67', '7688c1', '79011a', '7eb666', '7fcee9', '87174c', '8db7dd', 'a03db7', 'bc3908', 'c85fdf', 'c8e721', 'd8836b', 'e8919c', 'f2c8aa']\n",
      "Val subjects   (n=8): ['07285e', '364459', '3b2403', '48fd62', '54ee6e', '66341b', '93f49f', 'd9312a']\n",
      "Train windows: 15909, Val windows: 3971\n",
      "[Epoch 01] Train Loss=0.6375, Acc: 0.6219, F1=0.4392 | Val Loss=0.5924, Acc: 0.6464 F1=0.3217\n",
      "[Epoch 02] Train Loss=0.5114, Acc: 0.7398, F1=0.7155 | Val Loss=0.4874, Acc: 0.7346 F1=0.2351\n",
      "[Epoch 03] Train Loss=0.4232, Acc: 0.7988, F1=0.7733 | Val Loss=0.4392, Acc: 0.7968 F1=0.2723\n",
      "[Epoch 04] Train Loss=0.3741, Acc: 0.8423, F1=0.8218 | Val Loss=0.4948, Acc: 0.7666 F1=0.3832\n",
      "[Epoch 05] Train Loss=0.3525, Acc: 0.8518, F1=0.8318 | Val Loss=0.4468, Acc: 0.7955 F1=0.3083\n",
      "[Epoch 06] Train Loss=0.3414, Acc: 0.8576, F1=0.8388 | Val Loss=0.4265, Acc: 0.8053 F1=0.3488\n",
      "[Epoch 07] Train Loss=0.3188, Acc: 0.8669, F1=0.8501 | Val Loss=0.4508, Acc: 0.7834 F1=0.3704\n",
      "[Epoch 08] Train Loss=0.3053, Acc: 0.8745, F1=0.8591 | Val Loss=0.3998, Acc: 0.8290 F1=0.2675\n",
      "[Epoch 09] Train Loss=0.2960, Acc: 0.8783, F1=0.8632 | Val Loss=0.4055, Acc: 0.8134 F1=0.3133\n",
      "[Epoch 10] Train Loss=0.3225, Acc: 0.8659, F1=0.8497 | Val Loss=0.3967, Acc: 0.8101 F1=0.3599\n",
      "[Epoch 11] Train Loss=0.3374, Acc: 0.8559, F1=0.8420 | Val Loss=0.4286, Acc: 0.7980 F1=0.3522\n",
      "Early stopping at epoch 11 (no val F1 improvement for 7 epochs).\n",
      "============================================================\n",
      "Fold 4/5\n",
      "Train subjects (n=26): ['07285e', '194d1d', '220a17', '231c3b', '251738', '2a39f8', '2c98f7', '2d57c2', '31d269', '364459', '3b2403', '48fd62', '4f13b4', '516a67', '54ee6e', '66341b', '7688c1', '7fcee9', '87174c', '93f49f', 'a03db7', 'bc3908', 'c85fdf', 'd9312a', 'e8919c', 'f2c8aa']\n",
      "Val subjects   (n=8): ['24a59d', '4b39ac', '4ca9b3', '79011a', '7eb666', '8db7dd', 'c8e721', 'd8836b']\n",
      "Train windows: 15916, Val windows: 3964\n",
      "[Epoch 01] Train Loss=0.6390, Acc: 0.6265, F1=0.3482 | Val Loss=0.5669, Acc: 0.6660 F1=0.5322\n",
      "[Epoch 02] Train Loss=0.5666, Acc: 0.6885, F1=0.5872 | Val Loss=0.5485, Acc: 0.6988 F1=0.2565\n",
      "[Epoch 03] Train Loss=0.5218, Acc: 0.7185, F1=0.6118 | Val Loss=0.5388, Acc: 0.7159 F1=0.4475\n",
      "[Epoch 04] Train Loss=0.4657, Acc: 0.7621, F1=0.6849 | Val Loss=0.5137, Acc: 0.6781 F1=0.5569\n",
      "[Epoch 05] Train Loss=0.4215, Acc: 0.7907, F1=0.7133 | Val Loss=0.4828, Acc: 0.7750 F1=0.5373\n",
      "[Epoch 06] Train Loss=0.3865, Acc: 0.8127, F1=0.7537 | Val Loss=0.4577, Acc: 0.7485 F1=0.6371\n",
      "[Epoch 07] Train Loss=0.3716, Acc: 0.8279, F1=0.7763 | Val Loss=0.4250, Acc: 0.8047 F1=0.6246\n",
      "[Epoch 08] Train Loss=0.3588, Acc: 0.8391, F1=0.7917 | Val Loss=0.4295, Acc: 0.8272 F1=0.7358\n",
      "[Epoch 09] Train Loss=0.3673, Acc: 0.8314, F1=0.7824 | Val Loss=0.4888, Acc: 0.7074 F1=0.5408\n",
      "[Epoch 10] Train Loss=0.3598, Acc: 0.8324, F1=0.7805 | Val Loss=0.4306, Acc: 0.7999 F1=0.6439\n",
      "[Epoch 11] Train Loss=0.3365, Acc: 0.8522, F1=0.8119 | Val Loss=0.3991, Acc: 0.8214 F1=0.6895\n",
      "[Epoch 12] Train Loss=0.3270, Acc: 0.8601, F1=0.8241 | Val Loss=0.3838, Acc: 0.8378 F1=0.7362\n",
      "[Epoch 13] Train Loss=0.3206, Acc: 0.8622, F1=0.8275 | Val Loss=0.3756, Acc: 0.8403 F1=0.7503\n",
      "[Epoch 14] Train Loss=0.3134, Acc: 0.8670, F1=0.8351 | Val Loss=0.3665, Acc: 0.8312 F1=0.7049\n",
      "[Epoch 15] Train Loss=0.3150, Acc: 0.8657, F1=0.8354 | Val Loss=0.3969, Acc: 0.8123 F1=0.6430\n",
      "[Epoch 16] Train Loss=0.3141, Acc: 0.8664, F1=0.8333 | Val Loss=0.3605, Acc: 0.8436 F1=0.7530\n",
      "[Epoch 17] Train Loss=0.3047, Acc: 0.8693, F1=0.8367 | Val Loss=0.3743, Acc: 0.8406 F1=0.7472\n",
      "[Epoch 18] Train Loss=0.2965, Acc: 0.8753, F1=0.8447 | Val Loss=0.3689, Acc: 0.8418 F1=0.7410\n",
      "[Epoch 19] Train Loss=0.2953, Acc: 0.8750, F1=0.8454 | Val Loss=0.3607, Acc: 0.8471 F1=0.7711\n",
      "[Epoch 20] Train Loss=0.2840, Acc: 0.8797, F1=0.8515 | Val Loss=0.3692, Acc: 0.8423 F1=0.7395\n",
      "[Epoch 21] Train Loss=0.2837, Acc: 0.8804, F1=0.8519 | Val Loss=0.3474, Acc: 0.8489 F1=0.7550\n",
      "[Epoch 22] Train Loss=0.2795, Acc: 0.8820, F1=0.8535 | Val Loss=0.3425, Acc: 0.8577 F1=0.7778\n",
      "[Epoch 23] Train Loss=0.2762, Acc: 0.8842, F1=0.8564 | Val Loss=0.3417, Acc: 0.8549 F1=0.7699\n",
      "[Epoch 24] Train Loss=0.2779, Acc: 0.8826, F1=0.8541 | Val Loss=0.3512, Acc: 0.8625 F1=0.7917\n",
      "[Epoch 25] Train Loss=0.2729, Acc: 0.8867, F1=0.8596 | Val Loss=0.3393, Acc: 0.8517 F1=0.7674\n",
      "[Epoch 26] Train Loss=0.2667, Acc: 0.8879, F1=0.8614 | Val Loss=0.3515, Acc: 0.8469 F1=0.7585\n",
      "[Epoch 27] Train Loss=0.2683, Acc: 0.8868, F1=0.8595 | Val Loss=0.3371, Acc: 0.8512 F1=0.7511\n",
      "[Epoch 28] Train Loss=0.2653, Acc: 0.8878, F1=0.8609 | Val Loss=0.3386, Acc: 0.8620 F1=0.7856\n",
      "[Epoch 29] Train Loss=0.2600, Acc: 0.8935, F1=0.8679 | Val Loss=0.3343, Acc: 0.8580 F1=0.7649\n",
      "[Epoch 30] Train Loss=0.2570, Acc: 0.8925, F1=0.8670 | Val Loss=0.3375, Acc: 0.8570 F1=0.7658\n",
      "[Epoch 31] Train Loss=0.2570, Acc: 0.8936, F1=0.8683 | Val Loss=0.3263, Acc: 0.8633 F1=0.7799\n",
      "Early stopping at epoch 31 (no val F1 improvement for 7 epochs).\n",
      "============================================================\n",
      "Fold 5/5\n",
      "Train subjects (n=26): ['07285e', '194d1d', '231c3b', '24a59d', '251738', '2a39f8', '2d57c2', '364459', '3b2403', '48fd62', '4b39ac', '4ca9b3', '4f13b4', '54ee6e', '66341b', '7688c1', '79011a', '7eb666', '87174c', '8db7dd', '93f49f', 'c85fdf', 'c8e721', 'd8836b', 'd9312a', 'f2c8aa']\n",
      "Val subjects   (n=8): ['220a17', '2c98f7', '31d269', '516a67', '7fcee9', 'a03db7', 'bc3908', 'e8919c']\n",
      "Train windows: 15919, Val windows: 3961\n",
      "[Epoch 01] Train Loss=0.6541, Acc: 0.5991, F1=0.4149 | Val Loss=0.5945, Acc: 0.5635 F1=0.3453\n",
      "[Epoch 02] Train Loss=0.5368, Acc: 0.7211, F1=0.6933 | Val Loss=0.6134, Acc: 0.6385 F1=0.3998\n",
      "[Epoch 03] Train Loss=0.4400, Acc: 0.7789, F1=0.7502 | Val Loss=0.4762, Acc: 0.7662 F1=0.4945\n",
      "[Epoch 04] Train Loss=0.4062, Acc: 0.8002, F1=0.7697 | Val Loss=0.4087, Acc: 0.8490 F1=0.5818\n",
      "[Epoch 05] Train Loss=0.3910, Acc: 0.8272, F1=0.7968 | Val Loss=0.4864, Acc: 0.7763 F1=0.5371\n",
      "[Epoch 06] Train Loss=0.3674, Acc: 0.8399, F1=0.8123 | Val Loss=0.4705, Acc: 0.8054 F1=0.5705\n",
      "[Epoch 07] Train Loss=0.3585, Acc: 0.8497, F1=0.8274 | Val Loss=0.6336, Acc: 0.7107 F1=0.4699\n",
      "[Epoch 08] Train Loss=0.3500, Acc: 0.8481, F1=0.8288 | Val Loss=0.4218, Acc: 0.8170 F1=0.5864\n",
      "[Epoch 09] Train Loss=0.3598, Acc: 0.8389, F1=0.8112 | Val Loss=0.4715, Acc: 0.8048 F1=0.5787\n",
      "[Epoch 10] Train Loss=0.3385, Acc: 0.8564, F1=0.8363 | Val Loss=0.4764, Acc: 0.7854 F1=0.5484\n",
      "[Epoch 11] Train Loss=0.3288, Acc: 0.8607, F1=0.8409 | Val Loss=0.4450, Acc: 0.8023 F1=0.5724\n",
      "[Epoch 12] Train Loss=0.3126, Acc: 0.8697, F1=0.8519 | Val Loss=0.4132, Acc: 0.8245 F1=0.5938\n",
      "[Epoch 13] Train Loss=0.3103, Acc: 0.8698, F1=0.8525 | Val Loss=0.4413, Acc: 0.8086 F1=0.5817\n",
      "[Epoch 14] Train Loss=0.3049, Acc: 0.8703, F1=0.8528 | Val Loss=0.4513, Acc: 0.7998 F1=0.5711\n",
      "[Epoch 15] Train Loss=0.3067, Acc: 0.8692, F1=0.8510 | Val Loss=0.4149, Acc: 0.8205 F1=0.5911\n",
      "[Epoch 16] Train Loss=0.3022, Acc: 0.8735, F1=0.8561 | Val Loss=0.4370, Acc: 0.8107 F1=0.5829\n",
      "[Epoch 17] Train Loss=0.2946, Acc: 0.8772, F1=0.8603 | Val Loss=0.4290, Acc: 0.8155 F1=0.5858\n",
      "[Epoch 18] Train Loss=0.2932, Acc: 0.8768, F1=0.8602 | Val Loss=0.4191, Acc: 0.8208 F1=0.5838\n",
      "[Epoch 19] Train Loss=0.2910, Acc: 0.8769, F1=0.8603 | Val Loss=0.4089, Acc: 0.8250 F1=0.5902\n",
      "Early stopping at epoch 19 (no val F1 improvement for 7 epochs).\n",
      "üìÅ Saved best model for Fold 0 ‚Üí all_best_models\\best_model_fold_0.pt\n",
      "üìÅ Saved best model for Fold 1 ‚Üí all_best_models\\best_model_fold_1.pt\n",
      "üìÅ Saved best model for Fold 2 ‚Üí all_best_models\\best_model_fold_2.pt\n",
      "üìÅ Saved best model for Fold 3 ‚Üí all_best_models\\best_model_fold_3.pt\n",
      "üìÅ Saved best model for Fold 4 ‚Üí all_best_models\\best_model_fold_4.pt\n",
      "‚úÖ Fold metrics saved to Excel: fog_cv_folds.xlsx\n",
      "‚úÖ Summary metrics saved to Excel: fog_cv_summary.xlsx\n",
      "üèÜ Global best model (Fold 0) saved ‚Üí best_fog_model.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cv_results = cross_validate_patient_independent(\n",
    "    train_df,\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    input_dim=3,\n",
    "    hidden_dim=64,\n",
    "    num_layers=1,\n",
    "    bidirectional=False,\n",
    "    dropout=0.2,\n",
    "    batch_size=16,\n",
    "    num_epochs=50,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    early_stopping_patience=7,\n",
    "    device=None,  # auto: cuda if available else cpu\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e820ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, average_precision_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_models_on_test_ensemble(\n",
    "    test_df: pd.DataFrame,\n",
    "    model_paths: List[str],\n",
    "    batch_size: int = 32,\n",
    "    device: str = None,\n",
    "    fold_weights: List[float] = None   # OPTIONAL for weighted voting\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate an ensemble of models on the test set using:\n",
    "        - Soft voting (default)\n",
    "        - Hard majority voting\n",
    "        - Optional weighted voting\n",
    "\n",
    "    Args:\n",
    "        test_df        : Test dataframe\n",
    "        model_paths    : List of paths to saved fold models\n",
    "        batch_size     : Test batch size\n",
    "        device         : 'cpu' or 'cuda'\n",
    "        fold_weights   : Optional weights per fold (e.g. fold F1)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with all ensemble metrics and voting predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Device Setup\n",
    "    # -----------------------------\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # -----------------------------\n",
    "    # DataLoader\n",
    "    # -----------------------------\n",
    "    test_loader = make_dataloader(test_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # collect predictions from each model\n",
    "    prob_list = []     # soft voting\n",
    "    hard_list = []     # hard voting\n",
    "    targets_list = []\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load each model and predict\n",
    "    # -----------------------------\n",
    "    for idx, model_path in enumerate(model_paths):\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\" Model not found: {model_path}\")\n",
    "\n",
    "        print(f\" Loading model: {model_path}\")\n",
    "\n",
    "        model = LSTMTransformer().to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        fold_probs = []\n",
    "        fold_hard = []\n",
    "        fold_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                logits = model(X)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "                fold_probs.extend(probs)\n",
    "                fold_hard.extend(preds)\n",
    "                fold_targets.extend(y.cpu().numpy().astype(int))\n",
    "\n",
    "        prob_list.append(np.array(fold_probs))\n",
    "        hard_list.append(np.array(fold_hard))\n",
    "        targets_list = fold_targets     # same for all folds\n",
    "\n",
    "    prob_matrix = np.vstack(prob_list)     # shape: (num_models, N)\n",
    "    hard_matrix = np.vstack(hard_list)     # shape: (num_models, N)\n",
    "    ground_truth = np.array(targets_list)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Voting Methods\n",
    "    # -----------------------------\n",
    "\n",
    "    # SOFT VOTING (best default)\n",
    "    soft_probs = prob_matrix.mean(axis=0)\n",
    "    soft_preds = (soft_probs >= 0.5).astype(int)\n",
    "\n",
    "    # HARD VOTING\n",
    "    hard_preds = np.round(hard_matrix.mean(axis=0)).astype(int)\n",
    "\n",
    "    # WEIGHTED VOTING (if provided)\n",
    "    if fold_weights is not None:\n",
    "        w = np.array(fold_weights).reshape(-1, 1)\n",
    "        weighted_probs = (prob_matrix * w).sum(axis=0) / w.sum()\n",
    "        weighted_preds = (weighted_probs >= 0.5).astype(int)\n",
    "    else:\n",
    "        weighted_preds = None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Metric Function\n",
    "    # -----------------------------\n",
    "    def compute_metrics(preds, probs=None):\n",
    "        return {\n",
    "            \"loss\": criterion(\n",
    "                torch.tensor(preds, dtype=torch.float32),\n",
    "                torch.tensor(ground_truth, dtype=torch.float32)\n",
    "            ).item(),\n",
    "            \"accuracy\": accuracy_score(ground_truth, preds),\n",
    "            \"precision\": precision_score(ground_truth, preds, zero_division=0),\n",
    "            \"recall\": recall_score(ground_truth, preds, zero_division=0),\n",
    "            \"f1\": f1_score(ground_truth, preds, zero_division=0),\n",
    "            \"roc_auc\": roc_auc_score(ground_truth, probs) if probs is not None else None,\n",
    "            \"pr_auc\": average_precision_score(ground_truth, probs) if probs is not None else None,\n",
    "            \"confusion_matrix\": confusion_matrix(ground_truth, preds)\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Compute Metrics\n",
    "    # -----------------------------\n",
    "    metrics_soft = compute_metrics(soft_preds, soft_probs)\n",
    "    metrics_hard = compute_metrics(hard_preds, soft_probs)\n",
    "    metrics_weighted = compute_metrics(weighted_preds, weighted_probs) if weighted_preds is not None else None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Print Results\n",
    "    # -----------------------------\n",
    "    print(\"\\n\\n **SOFT VOTING RESULTS**\")\n",
    "    for k, v in metrics_soft.items():\n",
    "        if k != \"confusion_matrix\":\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    print(\"\\n **HARD VOTING RESULTS**\")\n",
    "    for k, v in metrics_hard.items():\n",
    "        if k != \"confusion_matrix\":\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    if metrics_weighted is not None:\n",
    "        print(\"\\n **WEIGHTED VOTING RESULTS**\")\n",
    "        for k, v in metrics_weighted.items():\n",
    "            if k != \"confusion_matrix\":\n",
    "                print(f\"{k}: {v}\")\n",
    "\n",
    "    return {\n",
    "        \"metrics_soft\": metrics_soft,\n",
    "        \"metrics_hard\": metrics_hard,\n",
    "        \"metrics_weighted\": metrics_weighted,\n",
    "        \"soft_preds\": soft_preds,\n",
    "        \"soft_probs\": soft_probs,\n",
    "        \"hard_preds\": hard_preds,\n",
    "        \"weighted_preds\": weighted_preds\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828f2b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\all_best_models\\\\best_model_fold_0.pt',\n",
       " '.\\\\all_best_models\\\\best_model_fold_1.pt',\n",
       " '.\\\\all_best_models\\\\best_model_fold_2.pt',\n",
       " '.\\\\all_best_models\\\\best_model_fold_3.pt',\n",
       " '.\\\\all_best_models\\\\best_model_fold_4.pt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = '.\\\\all_best_models\\\\'\n",
    "file_paths = []\n",
    "for root, _, files in os.walk(directory_path):\n",
    "    for file in files:\n",
    "        file_paths.append(os.path.join(root, file))\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e30d0a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading model: .\\all_best_models\\best_model_fold_0.pt\n",
      "üì• Loading model: .\\all_best_models\\best_model_fold_1.pt\n",
      "üì• Loading model: .\\all_best_models\\best_model_fold_2.pt\n",
      "üì• Loading model: .\\all_best_models\\best_model_fold_3.pt\n",
      "üì• Loading model: .\\all_best_models\\best_model_fold_4.pt\n",
      "\n",
      "\n",
      "üéØ **SOFT VOTING RESULTS**\n",
      "loss: 0.7093257904052734\n",
      "accuracy: 0.8680485338725986\n",
      "precision: 0.5307262569832403\n",
      "recall: 0.6713780918727915\n",
      "f1: 0.592823712948518\n",
      "roc_auc: 0.8776467890386399\n",
      "pr_auc: 0.5892170250248322\n",
      "\n",
      "üó≥Ô∏è **HARD VOTING RESULTS**\n",
      "loss: 0.7112265825271606\n",
      "accuracy: 0.8640040444893832\n",
      "precision: 0.5196629213483146\n",
      "recall: 0.6537102473498233\n",
      "f1: 0.5790297339593115\n",
      "roc_auc: 0.8776467890386399\n",
      "pr_auc: 0.5892170250248322\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_models_on_test_ensemble(\n",
    "    test_df=test_df,\n",
    "    model_paths= file_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71094667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üó≥Ô∏è **HARD VOTING RESULTS**\n",
    "# loss: 0.7022083401679993\n",
    "# accuracy: 0.8842264914054601\n",
    "# precision: 0.5721925133689839\n",
    "# recall: 0.7561837455830389\n",
    "# f1: 0.6514459665144596\n",
    "# roc_auc: 0.9207417367647519\n",
    "# pr_auc: 0.708372352847281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c584483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve\n",
    "\n",
    "\n",
    "def save_and_plot_ensemble_results(\n",
    "    eval_results: Dict[str, Any],\n",
    "    ground_truth: np.ndarray,\n",
    "    output_folder: str = \"ensemble_results/\",\n",
    "    model_output_path: str = \"ensemble_final_model.pt\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates plots (confusion matrix, ROC, PR curve),\n",
    "    saves predictions, and exports the ensemble model.\n",
    "\n",
    "    Args:\n",
    "        eval_results      : Output of evaluate_models_on_test_ensemble()\n",
    "        ground_truth      : Numpy array of true labels\n",
    "        output_folder     : Directory to save images & CSV\n",
    "        model_output_path : File to save final ensemble soft-voting model weights\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract predictions\n",
    "    # -----------------------------\n",
    "    soft_probs = eval_results[\"soft_probs\"]\n",
    "    soft_preds = eval_results[\"soft_preds\"]\n",
    "    hard_preds = eval_results[\"hard_preds\"]\n",
    "    weighted_preds = eval_results[\"weighted_preds\"]\n",
    "\n",
    "    # =============================\n",
    "    #  1. Save predictions to CSV\n",
    "    # =============================\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"y_true\": ground_truth,\n",
    "        \"soft_prob\": soft_probs,\n",
    "        \"soft_pred\": soft_preds,\n",
    "        \"hard_pred\": hard_preds,\n",
    "        \"weighted_pred\": weighted_preds if weighted_preds is not None else np.nan\n",
    "    })\n",
    "\n",
    "    csv_path = os.path.join(output_folder, \"ensemble_predictions.csv\")\n",
    "    pred_df.to_csv(csv_path, index=False)\n",
    "    print(f\" Predictions saved to: {csv_path}\")\n",
    "\n",
    "    # =============================\n",
    "    #  2. Confusion Matrix Plot\n",
    "    # =============================\n",
    "    cm = confusion_matrix(ground_truth, soft_preds)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.imshow(cm, cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks([0, 1])\n",
    "    plt.yticks([0, 1])\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, str(cm[i, j]), ha='center', va='center', color='red')\n",
    "\n",
    "    cm_path = os.path.join(output_folder, \"confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\" Confusion Matrix saved to: {cm_path}\")\n",
    "\n",
    "    # =============================\n",
    "    #  3. ROC Curve Plot\n",
    "    # =============================\n",
    "    fpr, tpr, _ = roc_curve(ground_truth, soft_probs)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, label=\"ROC\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Soft Voting)\")\n",
    "    plt.legend()\n",
    "\n",
    "    roc_path = os.path.join(output_folder, \"roc_curve.png\")\n",
    "    plt.savefig(roc_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\" ROC Curve saved to: {roc_path}\")\n",
    "\n",
    "    # =============================\n",
    "    #  4. Precision‚ÄìRecall Curve\n",
    "    # =============================\n",
    "    precision, recall, _ = precision_recall_curve(ground_truth, soft_probs)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(recall, precision, label=\"Precision‚ÄìRecall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision‚ÄìRecall Curve (Soft Voting)\")\n",
    "    plt.legend()\n",
    "\n",
    "    pr_path = os.path.join(output_folder, \"pr_curve.png\")\n",
    "    plt.savefig(pr_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\" Precision‚ÄìRecall Curve saved to: {pr_path}\")\n",
    "\n",
    "    # =============================\n",
    "    #  5. Export Final Ensemble Model\n",
    "    # =============================\n",
    "\n",
    "    \"\"\"\n",
    "    Ensemble model: soft-voting means averaging probabilities.\n",
    "    You cannot save a single PyTorch state dict unless we create\n",
    "    a small wrapper module below.\n",
    "    \"\"\"\n",
    "\n",
    "    class SoftVotingEnsemble(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, prob_list):\n",
    "            \"\"\"\n",
    "            prob_list: tensor shape (num_models, batch_size)\n",
    "            \"\"\"\n",
    "            return prob_list.mean(dim=0)\n",
    "\n",
    "    ensemble_model = SoftVotingEnsemble()\n",
    "    torch.save(ensemble_model.state_dict(), model_output_path)\n",
    "\n",
    "    print(f\"Final Ensemble Model saved to: {model_output_path}\")\n",
    "\n",
    "    print(\"\\n ALL RESULTS SAVED SUCCESSFULLY!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1af3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "1973    0\n",
       "1974    0\n",
       "1975    0\n",
       "1976    0\n",
       "1977    0\n",
       "Name: window_label, Length: 1978, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['window_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b793e153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Predictions saved to: ensemble_results/ensemble_predictions.csv\n",
      "üìä Confusion Matrix saved to: ensemble_results/confusion_matrix.png\n",
      "üìà ROC Curve saved to: ensemble_results/roc_curve.png\n",
      "üìâ Precision‚ÄìRecall Curve saved to: ensemble_results/pr_curve.png\n",
      "üß† Final Ensemble Model saved to: ensemble_final_soft_voting.pt\n",
      "\n",
      "üéâ ALL RESULTS SAVED SUCCESSFULLY!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ground_truth = test_df['window_label'].values  # or however your labels stored\n",
    "\n",
    "save_and_plot_ensemble_results(\n",
    "    eval_results=results,\n",
    "    ground_truth=ground_truth,\n",
    "    output_folder=\"ensemble_results/\",\n",
    "    model_output_path=\"ensemble_final_soft_voting.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b285025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
