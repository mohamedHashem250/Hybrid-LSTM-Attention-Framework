{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c584483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "\n",
    "def plot_model_comparisons(\n",
    "    folder_path: str,\n",
    "    prob_column: str = \"soft_prob\",\n",
    "    true_column: str = \"y_true\",\n",
    "    roc_output: str = \"comparison_roc.png\",\n",
    "    pr_output: str = \"comparison_pr.png\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads all CSV prediction files in a folder and plots:\n",
    "        - Combined ROC Curve\n",
    "        - Combined Precision‚ÄìRecall Curve\n",
    "\n",
    "    Args:\n",
    "        folder_path: directory containing CSV files (one per model)\n",
    "        prob_column: column name for predicted probabilities\n",
    "        true_column: column name for true labels\n",
    "        roc_output: filename for ROC comparison plot\n",
    "        pr_output: filename for PR comparison plot\n",
    "    \"\"\"\n",
    "\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "    if len(csv_files) == 0:\n",
    "        raise ValueError(\"No CSV files found in the folder.\")\n",
    "\n",
    "    print(f\"üìÅ Found {len(csv_files)} CSV files to compare.\")\n",
    "\n",
    "    # --------------------------\n",
    "    # 1. ROC COMPARISON PLOT\n",
    "    # --------------------------\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    for file in csv_files:\n",
    "        path = os.path.join(folder_path, file)\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        if true_column not in df or prob_column not in df:\n",
    "            print(f\"‚ö† Skipping {file}: required columns missing\")\n",
    "            continue\n",
    "\n",
    "        y_true = df[true_column].values\n",
    "        y_prob = df[prob_column].values\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "\n",
    "        model_name = os.path.splitext(file)[0]\n",
    "        plt.plot(fpr, tpr, label=f\"{model_name} (AUC={auc_score:.3f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "\n",
    "    plt.title(\"ROC Curve Comparison Across Models\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    save_roc = os.path.join(folder_path, roc_output)\n",
    "    plt.savefig(save_roc, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\" ROC comparison plot saved at: {save_roc}\")\n",
    "\n",
    "    # --------------------------\n",
    "    # 2. PRECISION‚ÄìRECALL COMPARISON\n",
    "    # --------------------------\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    for file in csv_files:\n",
    "        path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        if true_column not in df or prob_column not in df:\n",
    "            print(f\"‚ö† Skipping {file}: required columns missing\")\n",
    "            continue\n",
    "\n",
    "        y_true = df[true_column].values\n",
    "        y_prob = df[prob_column].values\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "        avg_precision = average_precision_score(y_true, y_prob)\n",
    "\n",
    "        model_name = os.path.splitext(file)[0]\n",
    "        plt.plot(recall, precision, label=f\"{model_name} (AP={avg_precision:.3f})\")\n",
    "\n",
    "    plt.title(\"Precision‚ÄìRecall Curve Comparison Across Models\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    save_pr = os.path.join(folder_path, pr_output)\n",
    "    plt.savefig(save_pr, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\" Precision‚ÄìRecall comparison plot saved at: {save_pr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485986d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    log_loss\n",
    ")\n",
    "\n",
    "\n",
    "def plot_model_comparisons(\n",
    "    folder_path: str,\n",
    "    csv_files, \n",
    "    prob_column: str = \"soft_prob\",\n",
    "    true_column: str = \"y_true\",\n",
    "    roc_output: str = \"comparison_roc.png\",\n",
    "    pr_output: str = \"comparison_pr.png\",\n",
    "    excel_output: str = \"model_comparison_metrics.xlsx\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads all CSV prediction files in a folder and plots:\n",
    "        - Combined ROC Curve\n",
    "        - Combined Precision‚ÄìRecall Curve\n",
    "        - Saves Excel with all metrics per model\n",
    "\n",
    "    Args:\n",
    "        folder_path : directory containing CSV files (one per model)\n",
    "    \"\"\"\n",
    "\n",
    "    #csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "    if len(csv_files) == 0:\n",
    "        raise ValueError(\"No CSV files found in the folder.\")\n",
    "\n",
    "    print(f\"üìÅ Found {len(csv_files)} CSV files to compare.\\n\")\n",
    "\n",
    "    # Store metrics for Excel\n",
    "    metrics_list = []\n",
    "\n",
    "    # --------------------------\n",
    "    # 1. ROC COMPARISON PLOT\n",
    "    # --------------------------\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    for file in csv_files:\n",
    "        path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        if true_column not in df or prob_column not in df:\n",
    "            print(f\"‚ö† Skipping {file}: missing required columns\")\n",
    "            continue\n",
    "\n",
    "        model_name = os.path.splitext(file)[0]\n",
    "        y_true = df[true_column].values\n",
    "        y_prob = df[prob_column].values\n",
    "\n",
    "        # Compute ROC\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, label=f\"{model_name} (AUC={auc_score:.3f})\")\n",
    "\n",
    "        # Compute metrics for Excel\n",
    "        y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "        metrics_list.append({\n",
    "            \"model_name\": model_name,\n",
    "            \"AUC\": auc_score,\n",
    "            \"Average Precision\": average_precision_score(y_true, y_prob),\n",
    "            \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"F1 Score\": f1_score(y_true, y_pred, zero_division=0),\n",
    "            \"Log Loss\": log_loss(y_true, y_prob, labels=[0,1])\n",
    "        })\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.title(\"ROC Curve Comparison Across Models\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    roc_path = os.path.join(folder_path, roc_output)\n",
    "    plt.savefig(roc_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"ROC comparison saved ‚Üí {roc_path}\")\n",
    "\n",
    "    # --------------------------\n",
    "    # 2. PR CURVE COMPARISON\n",
    "    # --------------------------\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    for file in csv_files:\n",
    "        path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        if true_column not in df or prob_column not in df:\n",
    "            continue\n",
    "\n",
    "        model_name = os.path.splitext(file)[0]\n",
    "        y_true = df[true_column].values\n",
    "        y_prob = df[prob_column].values\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "        avg_precision = average_precision_score(y_true, y_prob)\n",
    "\n",
    "        plt.plot(recall, precision, label=f\"{model_name} (AP={avg_precision:.3f})\")\n",
    "\n",
    "    plt.title(\"Precision‚ÄìRecall Curve Comparison\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    pr_path = os.path.join(folder_path, pr_output)\n",
    "    plt.savefig(pr_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\" PR comparison saved ‚Üí {pr_path}\")\n",
    "\n",
    "    # --------------------------\n",
    "    # 3. Excel File with Metrics\n",
    "    # --------------------------\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    excel_path = os.path.join(folder_path, excel_output)\n",
    "    metrics_df.to_excel(excel_path, index=False)\n",
    "\n",
    "    print(f\" Excel metrics saved ‚Üí {excel_path}\\n\")\n",
    "    print(metrics_df)\n",
    "\n",
    "    return metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5c31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 6 CSV files to compare.\n",
      "\n",
      "üìà ROC comparison saved ‚Üí C:\\Users\\Student\\Desktop\\Abouhashem\\DeepLearningProject\\OrganizedWork\\Allmetrics\\comparison_roc.png\n",
      "üìâ PR comparison saved ‚Üí C:\\Users\\Student\\Desktop\\Abouhashem\\DeepLearningProject\\OrganizedWork\\Allmetrics\\comparison_pr.png\n",
      "üìò Excel metrics saved ‚Üí C:\\Users\\Student\\Desktop\\Abouhashem\\DeepLearningProject\\OrganizedWork\\Allmetrics\\model_comparison_metrics.xlsx\n",
      "\n",
      "          model_name       AUC  Average Precision  Accuracy  Precision  \\\n",
      "0              1DCNN  0.867521           0.552674  0.861982   0.515723   \n",
      "1               LSTM  0.877647           0.589217  0.868049   0.530726   \n",
      "2                TCN  0.903697           0.640726  0.881699   0.574924   \n",
      "3  1DCNN_Transformer  0.903405           0.638901  0.856421   0.498851   \n",
      "4    TCN_Transformer  0.910537           0.686390  0.894843   0.622150   \n",
      "5   LSTM_Transformer  0.922662           0.716490  0.897877   0.611570   \n",
      "\n",
      "     Recall  F1 Score  Log Loss  \n",
      "0  0.579505  0.545757  0.334784  \n",
      "1  0.671378  0.592824  0.350333  \n",
      "2  0.664311  0.616393  0.287344  \n",
      "3  0.766784  0.604457  0.344762  \n",
      "4  0.674912  0.647458  0.260551  \n",
      "5  0.784452  0.687307  0.273261  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1DCNN</td>\n",
       "      <td>0.867521</td>\n",
       "      <td>0.552674</td>\n",
       "      <td>0.861982</td>\n",
       "      <td>0.515723</td>\n",
       "      <td>0.579505</td>\n",
       "      <td>0.545757</td>\n",
       "      <td>0.334784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.877647</td>\n",
       "      <td>0.589217</td>\n",
       "      <td>0.868049</td>\n",
       "      <td>0.530726</td>\n",
       "      <td>0.671378</td>\n",
       "      <td>0.592824</td>\n",
       "      <td>0.350333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCN</td>\n",
       "      <td>0.903697</td>\n",
       "      <td>0.640726</td>\n",
       "      <td>0.881699</td>\n",
       "      <td>0.574924</td>\n",
       "      <td>0.664311</td>\n",
       "      <td>0.616393</td>\n",
       "      <td>0.287344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1DCNN_Transformer</td>\n",
       "      <td>0.903405</td>\n",
       "      <td>0.638901</td>\n",
       "      <td>0.856421</td>\n",
       "      <td>0.498851</td>\n",
       "      <td>0.766784</td>\n",
       "      <td>0.604457</td>\n",
       "      <td>0.344762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCN_Transformer</td>\n",
       "      <td>0.910537</td>\n",
       "      <td>0.686390</td>\n",
       "      <td>0.894843</td>\n",
       "      <td>0.622150</td>\n",
       "      <td>0.674912</td>\n",
       "      <td>0.647458</td>\n",
       "      <td>0.260551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LSTM_Transformer</td>\n",
       "      <td>0.922662</td>\n",
       "      <td>0.716490</td>\n",
       "      <td>0.897877</td>\n",
       "      <td>0.611570</td>\n",
       "      <td>0.784452</td>\n",
       "      <td>0.687307</td>\n",
       "      <td>0.273261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model_name       AUC  Average Precision  Accuracy  Precision  \\\n",
       "0              1DCNN  0.867521           0.552674  0.861982   0.515723   \n",
       "1               LSTM  0.877647           0.589217  0.868049   0.530726   \n",
       "2                TCN  0.903697           0.640726  0.881699   0.574924   \n",
       "3  1DCNN_Transformer  0.903405           0.638901  0.856421   0.498851   \n",
       "4    TCN_Transformer  0.910537           0.686390  0.894843   0.622150   \n",
       "5   LSTM_Transformer  0.922662           0.716490  0.897877   0.611570   \n",
       "\n",
       "     Recall  F1 Score  Log Loss  \n",
       "0  0.579505  0.545757  0.334784  \n",
       "1  0.671378  0.592824  0.350333  \n",
       "2  0.664311  0.616393  0.287344  \n",
       "3  0.766784  0.604457  0.344762  \n",
       "4  0.674912  0.647458  0.260551  \n",
       "5  0.784452  0.687307  0.273261  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_path = 'C:\\\\Users\\\\Student\\\\Desktop\\\\Abouhashem\\\\DeepLearningProject\\\\OrganizedWork\\\\Allmetrics\\\\'\n",
    "csv_files = ['1DCNN.csv','LSTM.csv','TCN.csv','1DCNN_Transformer.csv','TCN_Transformer.csv','LSTM_Transformer.csv']\n",
    "plot_model_comparisons(files_path, csv_files )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
